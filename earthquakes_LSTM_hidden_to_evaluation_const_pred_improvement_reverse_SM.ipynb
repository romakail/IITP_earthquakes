{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "import random\n",
    "import datetime as dt\n",
    "\n",
    "# Results presentation\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NN related stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_INFO = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'Experiments/kernel_size_15/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e77bca062b1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Experiments/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mEXPERIMENT_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'kernel_size_15/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXPERIMENT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'Experiments/kernel_size_15/'"
     ]
    }
   ],
   "source": [
    "if (SAVE_INFO == True):\n",
    "    DATA_DIR = 'Experiments/'\n",
    "    EXPERIMENT_DIR = DATA_DIR + 'kernel_size_15/'\n",
    "    os.makedirs(EXPERIMENT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение датасета по дням и по клеткам в сетке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT_BORDER = 0\n",
    "RIGHT_BORDER = 2000\n",
    "DOWN_BORDER = 0\n",
    "UP_BORDER = 2500\n",
    "\n",
    "N_CELLS_HOR = 200\n",
    "N_CELLS_VER = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celled_data = torch.load(\"Data/celled_data_\"\n",
    "                         + str(N_CELLS_HOR)\n",
    "                         + \"x\"\n",
    "                         + str(N_CELLS_VER))\n",
    "print (celled_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_one_day_image (tensor, day):\n",
    "    plt.imshow (tensor[day].squeeze(0), cmap=plt.cm.afmhot_r)\n",
    "    plt.colorbar()\n",
    "    \n",
    "def show_one_day_quakes (tensor, day):\n",
    "    state = tensor[day].squeeze(0)\n",
    "    print (state.shape)\n",
    "    X = []\n",
    "    Y = []\n",
    "    M = []\n",
    "    for i in range(state.shape[0]):\n",
    "        for j in range(state.shape[1]):\n",
    "            if (state[i][j] != 0):\n",
    "                X.append(i)\n",
    "                Y.append(j)\n",
    "                M.append(state[i][j].item())\n",
    "    print (X)\n",
    "    print (Y)\n",
    "    print (M)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 10))\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    plt.axis([0, state.shape[0], 0, state.shape[1]])\n",
    "    axes.scatter(X, Y, s=500, c=M, marker='.', cmap=plt.cm.Reds)\n",
    "#     plt.colorbar()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_one_day_quakes (celled_data, 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_ID = 6\n",
    "DEVICE = torch.device('cuda:%d' % DEVICE_ID)\n",
    "print (DEVICE)\n",
    "# torch.cuda.set_device(DEVICE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_accuracy (input, target, threshold):\n",
    "    true = ((input>threshold) == target)\n",
    "    sum = torch.sum (true.float())\n",
    "    return sum/input.shape[0]/input.shape[1]/input.shape[2]/input.shape[3]\n",
    "\n",
    "def my_precision (input, target, threshold):\n",
    "    TP = torch.sum (((input>threshold) * target      ).float())\n",
    "    FP = torch.sum (((input>threshold) * (1 - target)).float())\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "def my_recall (input, target, threshold):\n",
    "    TP = torch.sum ((     (input>threshold)  * target).float())\n",
    "    FN = torch.sum (((1 - (input>threshold)) * target).float())\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def my_precision_recall (input, target, threshold):\n",
    "    TP = torch.sum ((     (input>threshold)  * target      ).float())\n",
    "    FP = torch.sum ((     (input>threshold)  * (1 - target)).float())\n",
    "    FN = torch.sum (((1 - (input>threshold)) * target      ).float())\n",
    "#     print ('TP = ', TP.item(), 'FP = ', FP.item(), 'FN = ', FN.item(), 'N = ', input.shape[0])\n",
    "    return TP / (TP + FP), TP / (TP + FN)\n",
    "\n",
    "def my_precision_TPR_FPR (input, target, threshold):\n",
    "    TP = torch.sum ((     (input>threshold) .float() * target      ).float())\n",
    "    FP = torch.sum ((     (input>threshold) .float() * (1 - target)).float())\n",
    "    FN = torch.sum (((1 - (input>threshold)).float() * target      ).float())\n",
    "    TN = torch.sum (((1 - (input>threshold)).float() * (1 - target)).float())\n",
    "    return TP / (TP + FP), TP / (TP + FN), FP / (FP + TN)\n",
    "\n",
    "def my_TP_FN_FP_TN (input, target, threshold):\n",
    "    matrix = np.zeros((2, 2))\n",
    "    matrix[0, 0] = torch.sum ((     (input>threshold) .float() * target      ).float())\n",
    "    matrix[1, 0] = torch.sum ((     (input>threshold) .float() * (1 - target)).float())\n",
    "    matrix[0, 1] = torch.sum (((1 - (input>threshold)).float() * target      ).float())\n",
    "    matrix[1, 1] = torch.sum (((1 - (input>threshold)).float() * (1 - target)).float())\n",
    "    return matrix / np.sum(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим датасет\n",
    "#### (Может не влезть в оперативку (надо ~ 12Gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVED_DAYS = 64     # ~2 months\n",
    "DAYS_TO_PREDICT_AFTER  = 10\n",
    "DAYS_TO_PREDICT_BEFORE = 50\n",
    "TESTING_DAYS = 1000\n",
    "\n",
    "HEAVY_QUAKE_THRES = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val = (celled_data>HEAVY_QUAKE_THRES).float().mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (mean_val.shape)\n",
    "plt.imshow(mean_val[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarthquakeDataset_RNN_Train (Dataset):\n",
    "    def __init__(self, celled_data):\n",
    "        self.data = celled_data[0:\n",
    "                                (celled_data.shape[0] -\n",
    "                                 TESTING_DAYS)]\n",
    "        self.size = (self.data.shape[0] -\n",
    "                     DAYS_TO_PREDICT_BEFORE)\n",
    "        \n",
    "        print ('self.data :', self.data.shape)\n",
    "        print ('size      :', self.size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx],\n",
    "                torch.sum(self.data[(idx +\n",
    "                                     DAYS_TO_PREDICT_AFTER):\n",
    "                                    (idx +\n",
    "                                     DAYS_TO_PREDICT_BEFORE)] > HEAVY_QUAKE_THRES,\n",
    "                          dim=0,\n",
    "                          keepdim=True).squeeze(0) > 0)\n",
    "        \n",
    "\n",
    "class EarthquakeDataset_RNN_Test (Dataset):\n",
    "    def __init__(self, celled_data):\n",
    "        self.data = celled_data[(celled_data.shape[0] -\n",
    "                                 TESTING_DAYS):\n",
    "                                (celled_data.shape[0])]\n",
    "        self.size = (self.data.shape[0] -\n",
    "                     DAYS_TO_PREDICT_BEFORE)\n",
    "        \n",
    "        print ('self.data :', self.data.shape)\n",
    "        print ('size      :', self.size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[(idx)],\n",
    "                torch.sum(self.data[(idx +\n",
    "                                     DAYS_TO_PREDICT_AFTER):\n",
    "                                    (idx +\n",
    "                                     DAYS_TO_PREDICT_BEFORE)] > HEAVY_QUAKE_THRES,\n",
    "                          dim=0,\n",
    "                          keepdim=True).squeeze(0) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes_dataset_train = EarthquakeDataset_RNN_Train (celled_data)\n",
    "earthquakes_dataset_test  = EarthquakeDataset_RNN_Test  (celled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes_dataloader_train = DataLoader(earthquakes_dataset_train,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=1)\n",
    "\n",
    "earthquakes_dataloader_test  = DataLoader(earthquakes_dataset_test,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим саму сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.CONV  = nn.Conv2d    (in_channels,\n",
    "                                   out_channels,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=stride,\n",
    "                                   padding=padding,\n",
    "                                   bias=False)             # think about it later\n",
    "        \n",
    "        self.BNORM =nn.BatchNorm2d(out_channels,\n",
    "                                   eps=1e-05,\n",
    "                                   momentum=0.1,\n",
    "                                   affine=False)\n",
    "#         self.RELU  = nn.ReLU ()\n",
    "        \n",
    "#         self.MAXPOOL = nn.MaxPool2d(3,\n",
    "#                                     stride=1,\n",
    "#                                     padding=1,\n",
    "#                                     dilation=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print ('sizeof(x) = ', x.size())\n",
    "        #print ('sizeof(xprev) = ', xprev.size())    \n",
    "        \n",
    "        x = self.CONV   (x)\n",
    "        x = self.BNORM  (x)\n",
    "#         x = self.RELU   (x)\n",
    "#         x = self.MAXPOOL(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "![LSTM](./img/LSTM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL_SIZE = 15\n",
    "PADDING     = int ((KERNEL_SIZE - 1) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell (nn.Module):\n",
    "    \n",
    "    def __init__ (self,\n",
    "                  mean_val,\n",
    "                  embedding_size=16,\n",
    "                  hidden_state_size=32,\n",
    "                  reverse_log_bias=0,\n",
    "                  device=torch.device('cpu')):\n",
    "        super(self.__class__,self).__init__()\n",
    "        \n",
    "        self.emb_size = embedding_size\n",
    "        self.hid_size = hidden_state_size\n",
    "        \n",
    "        self.mean_val = mean_val.to(device)\n",
    "        self.prior = torch.log(torch.cat([1-mean_val, mean_val], dim=0)).to(device).unsqueeze(0)\n",
    "        self.prior = self.prior + reverse_log_bias\n",
    "        \n",
    "#         self.embedding = ConvBlock (1, self.emb_size, kernel_size=3)\n",
    "        self.embedding = nn.Sequential(ConvBlock(1,\n",
    "                                                 self.emb_size,\n",
    "                                                 kernel_size=KERNEL_SIZE,\n",
    "                                                 padding=PADDING),\n",
    "                                       nn.ReLU(),\n",
    "                                       ConvBlock(self.emb_size,\n",
    "                                                 self.emb_size,\n",
    "                                                 kernel_size=KERNEL_SIZE,\n",
    "                                                 padding=PADDING))\n",
    "        self.hidden_to_result = ConvBlock(hidden_state_size, \n",
    "                                          2, \n",
    "                                          kernel_size=KERNEL_SIZE,\n",
    "                                          padding=PADDING)\n",
    "        \n",
    "        self.out_activation = nn.Softmax (dim=1)\n",
    "        \n",
    "        self.f_t = nn.Sequential (ConvBlock(self.hid_size + self.emb_size,\n",
    "                                            self.hid_size,\n",
    "                                            kernel_size=KERNEL_SIZE,\n",
    "                                            padding=PADDING),\n",
    "                                  nn.Sigmoid())\n",
    "        self.i_t = nn.Sequential (ConvBlock(self.hid_size + self.emb_size,\n",
    "                                            self.hid_size,\n",
    "                                            kernel_size=KERNEL_SIZE,\n",
    "                                            padding=PADDING),\n",
    "                                  nn.Sigmoid())\n",
    "        self.c_t = nn.Sequential (ConvBlock(self.hid_size + self.emb_size,\n",
    "                                            self.hid_size,\n",
    "                                            kernel_size=KERNEL_SIZE,\n",
    "                                            padding=PADDING),\n",
    "                                  nn.Tanh())\n",
    "        self.o_t = nn.Sequential (ConvBlock(self.hid_size + self.emb_size,\n",
    "                                            self.hid_size,\n",
    "                                            kernel_size=KERNEL_SIZE,\n",
    "                                            padding=PADDING),\n",
    "                                  nn.Sigmoid())\n",
    "        \n",
    "    def forward (self, x, prev_state):\n",
    "        (prev_c, prev_h) = prev_state\n",
    "        x_emb = self.embedding(x)\n",
    "        \n",
    "        x_and_h = torch.cat([prev_h, x_emb], dim=1)\n",
    "        \n",
    "        f_i = self.f_t(x_and_h)\n",
    "        i_i = self.i_t(x_and_h)\n",
    "        c_i = self.c_t(x_and_h)\n",
    "        o_i = self.o_t(x_and_h)\n",
    "        \n",
    "        next_c = prev_c * f_i + i_i * c_i\n",
    "        next_h = torch.tanh(next_c) * o_i\n",
    "        \n",
    "        assert prev_h.shape == next_h.shape\n",
    "        assert prev_c.shape == next_c.shape\n",
    "        \n",
    "        out = self.hidden_to_result(next_h)\n",
    "        \n",
    "        return (next_c, next_h), self.out_activation(self.prior + out)\n",
    "        \n",
    "    def init_state (self, batch_size, device=torch.device(\"cpu\")):\n",
    "        return (Variable(torch.zeros(batch_size,\n",
    "                                     self.hid_size,\n",
    "                                     N_CELLS_HOR,\n",
    "                                     N_CELLS_VER,\n",
    "                                     device=device)),\n",
    "                Variable(torch.zeros(batch_size,\n",
    "                                     self.hid_size,\n",
    "                                     N_CELLS_HOR,\n",
    "                                     N_CELLS_VER,\n",
    "                                     device=device)))\n",
    "               \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция тренеровки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network_RNN (RNN_cell,\n",
    "                       device,\n",
    "                       dataloader_train,\n",
    "                       n_cycles=1,\n",
    "                       learning_rate=0.0003,\n",
    "                       earthquake_weight=1.,\n",
    "                       lr_decay=1.):\n",
    "    \n",
    "    loss_massive = []\n",
    "    i = 0\n",
    "    \n",
    "    RNN_cell.to(device)\n",
    "    \n",
    "    weights = torch.tensor([1., earthquake_weight], dtype=torch.float).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weights)\n",
    "    \n",
    "    i = 0\n",
    "    for cycle in range(n_cycles):\n",
    "        \n",
    "        optimizer = torch.optim.Adam(RNN_cell.parameters(), lr=learning_rate)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        hid_state = RNN_cell.init_state(batch_size=1, device=device)\n",
    "#         for data in tqdm(dataloader_train):\n",
    "        for data in dataloader_train:\n",
    "            \n",
    "            inputs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "            \n",
    "#             print (\"inputs\", inputs.shape)\n",
    "#             print (\"hid_state\", hid_state.shape)\n",
    "            hid_state, outputs = RNN_cell.forward(inputs, hid_state)\n",
    "            \n",
    "            loss = criterion(outputs, labels.squeeze(1).long())\n",
    "#             loss = my_crossEntropy(weights, outputs, labels)\n",
    "            loss_massive.append(loss.item())\n",
    "#             loss.backward(retain_graph=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "#             print (\"day : \", i, \"loss : \", loss.item())\n",
    "            \n",
    "            if (type(hid_state) == tuple):\n",
    "                for elem in hid_state:\n",
    "                    elem.detach_()\n",
    "            else:\n",
    "                hid_state.detach_()\n",
    "            \n",
    "            if (i)%100==0:\n",
    "                clear_output(True)\n",
    "                print (\"Done :\", i, \"/\", dataloader_train.__len__())\n",
    "                plt.plot(loss_massive,label='loss')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "            i += 1\n",
    "        learning_rate /= lr_decay\n",
    "    return hid_state\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network_RNN_grad(RNN_cell,\n",
    "                           device,\n",
    "                           dataset_train,\n",
    "                           n_runs=1,\n",
    "                           run_len=10,\n",
    "                           batch_size=1,\n",
    "                           learning_rate=0.0003,\n",
    "                           earthquake_weight=1.,\n",
    "                           lr_decay=1.):\n",
    "    \n",
    "    loss_massive = []\n",
    "    i = 0\n",
    "    \n",
    "    RNN_cell.to(device)\n",
    "    \n",
    "    weights = torch.tensor([1., earthquake_weight], dtype=torch.float).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weights)\n",
    "    \n",
    "    i = 0\n",
    "    for run in range(n_runs):\n",
    "        \n",
    "        optimizer = torch.optim.Adam(RNN_cell.parameters(), lr=learning_rate)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        hid_state = RNN_cell.init_state(batch_size=1, device=device)\n",
    "        # Better figure out whether the hidden changes\n",
    "        \n",
    "        start = random.randint(0, \n",
    "                               dataset_train.__len__() - \n",
    "                               run_len - 1)\n",
    "        \n",
    "#         for data in dataloader_train:\n",
    "        for idx in range(start, start + run_len):\n",
    "            \n",
    "            inputs = dataset_train[idx][0].to(device).unsqueeze(0)\n",
    "            labels = dataset_train[idx][1].to(device)\n",
    "            \n",
    "            hid_state, outputs = RNN_cell.forward(inputs, hid_state)\n",
    "            \n",
    "#             loss = criterion(outputs, labels.squeeze(1).long())\n",
    "#             loss = my_crossEntropy(weights, outputs, labels)\n",
    "#             loss_massive.append(loss.item())\n",
    "#             loss.backward(retain_graph=True)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels.squeeze(1).long())\n",
    "        loss_massive.append(loss.item())\n",
    "        loss.backward()\n",
    "        if (run % batch_size == batch_size - 1):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "#             print (\"day : \", i, \"loss : \", loss.item())\n",
    "            \n",
    "#             if (type(hid_state) == tuple):\n",
    "#                 for elem in hid_state:\n",
    "#                     elem.detach_()\n",
    "#             else:\n",
    "#                 hid_state.detach_()\n",
    "            \n",
    "        if (i)%5==0:\n",
    "            clear_output(True)\n",
    "            print (\"Done :\", i, \"/\", dataset_train.__len__())\n",
    "            plt.plot(loss_massive,label='loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        i += 1\n",
    "        learning_rate /= lr_decay\n",
    "    return hid_state\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CYCLES = 1\n",
    "N_RUNS = 300\n",
    "RUN_LEN = 150\n",
    "BATCH_SIZE = 5\n",
    "LEARNING_RATE = 0.0003\n",
    "LR_DECAY = 10.\n",
    "EARTHQUAKE_WEIGHT = 1000.\n",
    "REVERSE_LOG_BIAS = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 16\n",
    "HID_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_cell = LSTMCell(mean_val,\n",
    "                    embedding_size    = EMB_SIZE,\n",
    "                    hidden_state_size = HID_SIZE,\n",
    "                    reverse_log_bias  = REVERSE_LOG_BIAS,\n",
    "                    device=DEVICE)\n",
    "hid_state = train_network_RNN (RNN_cell,\n",
    "                               DEVICE,\n",
    "                               earthquakes_dataloader_train,\n",
    "                               n_cycles=N_CYCLES,\n",
    "                               learning_rate=LEARNING_RATE,\n",
    "                               earthquake_weight=EARTHQUAKE_WEIGHT,\n",
    "                               lr_decay=LR_DECAY\n",
    "                               )\n",
    "train_network_RNN_grad(RNN_cell,\n",
    "                       DEVICE,\n",
    "                       earthquakes_dataset_train,\n",
    "                       n_runs=N_RUNS,\n",
    "                       run_len=RUN_LEN,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       learning_rate=LEARNING_RATE,\n",
    "                       earthquake_weight=EARTHQUAKE_WEIGHT,\n",
    "                       lr_decay=LR_DECAY\n",
    "                       )\n",
    "hid_state = train_network_RNN (RNN_cell,\n",
    "                               DEVICE,\n",
    "                               earthquakes_dataloader_train,\n",
    "                               n_cycles=N_CYCLES,\n",
    "                               learning_rate=LEARNING_RATE,\n",
    "                               earthquake_weight=EARTHQUAKE_WEIGHT,\n",
    "                               lr_decay=LR_DECAY\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def check_quality (RNN_cell,\n",
    "                   device,\n",
    "                   dataloader_test,\n",
    "                   hid_state,\n",
    "                   n_dots=501,\n",
    "                   info_file=None):\n",
    "    \n",
    "    prediction = torch.zeros(dataloader_test.__len__(),  N_CELLS_HOR, N_CELLS_VER,\n",
    "                             device=device,\n",
    "                             dtype=torch.float)\n",
    "    prediction.detach_()\n",
    "    target     = torch.zeros(dataloader_test.__len__(), N_CELLS_HOR, N_CELLS_VER,\n",
    "                             device=device,\n",
    "                             dtype=torch.float)\n",
    "    target.detach_()\n",
    "       \n",
    "    RNN_cell.to(device)\n",
    "\n",
    "    hid_state = hid_state\n",
    "    if (type(hid_state) == tuple):\n",
    "        for elem in hid_state:\n",
    "            elem.detach_()\n",
    "    else:\n",
    "        hid_state.detach_()\n",
    "        \n",
    "    i = 0\n",
    "    for data in tqdm(dataloader_test):\n",
    "\n",
    "        inputs = data[0].to(device)\n",
    "        labels = data[1].to(device).float()\n",
    "\n",
    "        hid_state, outputs = RNN_cell.forward(inputs, hid_state)\n",
    "        \n",
    "        prediction[i] = outputs[:, 1, :, :]\n",
    "        target    [i] = labels.squeeze(0)\n",
    "    \n",
    "        if (type(hid_state) == tuple):\n",
    "            for elem in hid_state:\n",
    "                elem.detach_()\n",
    "        else:\n",
    "            hid_state.detach_()\n",
    "        prediction.detach_()\n",
    "        target    .detach_()\n",
    "        i += 1\n",
    "        \n",
    "    assert prediction.shape == target.shape\n",
    "#     prediction = prediction [10:prediction.shape[0]]  # cutting peace of data because\n",
    "#     target     = target     [10:target    .shape[0]]  # hidden state might be not good\n",
    "    \n",
    "    print (\"ROC_AUC_score = \", end='')\n",
    "    ROC_AUC_score = roc_auc_score(np.array(target    .view(-1).cpu()),\n",
    "                                  np.array(prediction.view(-1).cpu()))\n",
    "    print (ROC_AUC_score)\n",
    "    if (SAVE_INFO):\n",
    "        print ('ROC_AUC               =', ROC_AUC_score, file=info_file)\n",
    "    \n",
    "    print (\"AVG_precision_score = \", end='')\n",
    "    AVG_precision_score = average_precision_score(np.array(target    .view(-1).cpu()),\n",
    "                                                  np.array(prediction.view(-1).cpu()))\n",
    "    print (AVG_precision_score)\n",
    "    if (SAVE_INFO):\n",
    "        print ('Average_precision     =', AVG_precision_score, file=info_file)\n",
    "        \n",
    "    print ('\\n=======================')\n",
    "    \n",
    "    for threshold in (0.2, 0.4, 0.6, 0.8):\n",
    "        print ('Threshold = ', threshold)\n",
    "        print ('-----------------------')\n",
    "        print (my_TP_FN_FP_TN(prediction, target, threshold))\n",
    "        print ('=======================')\n",
    "    \n",
    "    if SAVE_INFO:\n",
    "        print ('\\n=======================', file=info_file)\n",
    "    \n",
    "        for threshold in (0.2, 0.4, 0.6, 0.8):\n",
    "            print ('Threshold = ', threshold                    , file=info_file)\n",
    "            print ('-----------------------'                    , file=info_file)\n",
    "            print (my_TP_FN_FP_TN(prediction, target, threshold), file=info_file)\n",
    "            print ('======================='                    , file=info_file)\n",
    "    \n",
    "    threshold_massive = torch.linspace (0, 1, n_dots, dtype=torch.float, device=device)\n",
    "    \n",
    "#     precision = np.zeros(n_dots)\n",
    "#     recall    = np.zeros(n_dots)\n",
    "#     FPR       = np.zeros(n_dots)\n",
    "\n",
    "    precision_massive = []\n",
    "    recall_massive    = []\n",
    "    FPR_massive       = []\n",
    "    \n",
    "    for threshold in tqdm(threshold_massive):\n",
    "        precision, recall, FPR = my_precision_TPR_FPR(prediction, target, threshold)\n",
    "        precision_massive.append(precision.item())\n",
    "        recall_massive   .append(recall.item())\n",
    "        FPR_massive      .append(FPR.item())\n",
    "    \n",
    "    # plot 1 precision\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "    axes.plot(np.array(threshold_massive.cpu()), precision_massive, color='green', marker='^')\n",
    "\n",
    "    axes.set_xlabel('threshold')\n",
    "    axes.set_ylabel('precision')\n",
    "\n",
    "    if (SAVE_INFO == True):\n",
    "        plt.savefig(EXPERIMENT_DIR + 'Precision_from_threshold.png', format='png', dpi=100)\n",
    "    plt.show()\n",
    "    \n",
    "    # plot 2 recall\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "    axes.plot(np.array(threshold_massive.cpu()), recall_massive, color='green', marker='^')\n",
    "\n",
    "    axes.set_xlabel('threshold')\n",
    "    axes.set_ylabel('recall')\n",
    "    \n",
    "    if (SAVE_INFO == True):\n",
    "        plt.savefig(EXPERIMENT_DIR + 'Recall_from_threshold.png', format='png', dpi=100)\n",
    "    plt.show()\n",
    "    \n",
    "    # plot 3 ROC-curve\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "    axes.plot(FPR_massive, recall_massive, 'orange', marker = '^')\n",
    "    axes.plot (range(2), range(2), 'grey', ls='--')\n",
    "\n",
    "    axes.set_xlabel('FPR')\n",
    "    axes.set_ylabel('TPR (recall)')\n",
    "    axes.set_title('ROC-curve')\n",
    "\n",
    "    if (SAVE_INFO == True):\n",
    "        plt.savefig(EXPERIMENT_DIR + 'ROC_curve.png', format='png', dpi=100)\n",
    "    plt.show()\n",
    "    \n",
    "    # plot 4 precision-recall-curve\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "    axes.plot(recall_massive, precision_massive, 'orange', marker = '^')\n",
    "\n",
    "    axes.set_xlabel('Recall')\n",
    "    axes.set_ylabel('Precision')\n",
    "    axes.set_title('Precision_Recall_curve')\n",
    "\n",
    "    if (SAVE_INFO == True):\n",
    "        plt.savefig(EXPERIMENT_DIR + 'Precision_Recall_curve.png', format='png', dpi=100)\n",
    "    plt.show()\n",
    "    \n",
    "    return ROC_AUC_score, AVG_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_INFO_to_file(info_file):\n",
    "        \n",
    "    print ('LEFT_BORDER           =', LEFT_BORDER                           , file=info_file)\n",
    "    print ('RIGHT_BORDER          =', RIGHT_BORDER                          , file=info_file)\n",
    "    print ('DOWN_BORDER           =', DOWN_BORDER                           , file=info_file)\n",
    "    print ('UP_BORDER             =', UP_BORDER                             , file=info_file)\n",
    "    print ('N_CELLS_HOR           =', N_CELLS_HOR                           , file=info_file)\n",
    "    print ('N_CELLS_VER           =', N_CELLS_VER                           , file=info_file)\n",
    "    print (' '                                                              , file=info_file)\n",
    "    print ('OBSERVED_DAYS         =', OBSERVED_DAYS                         , file=info_file)\n",
    "    print ('DAYS_TO_PREDICT_AFTER =', DAYS_TO_PREDICT_AFTER                 , file=info_file)\n",
    "    print ('DAYS_TO_PREDICT_BEFOR =', DAYS_TO_PREDICT_BEFORE                , file=info_file)\n",
    "    print ('TESTING_DAYS          =', TESTING_DAYS                          , file=info_file)\n",
    "    print ('HEAVY_QUAKE_THRES     =', HEAVY_QUAKE_THRES                     , file=info_file)\n",
    "    print ('LEARNING_RATE         =', LEARNING_RATE                         , file=info_file)\n",
    "    print ('LR_DECAY              =', LR_DECAY                              , file=info_file)\n",
    "    print ('N_CYCLES              =', N_CYCLES                              , file=info_file)\n",
    "    print ('N_RUNS                =', N_RUNS                                , file=info_file)\n",
    "    print ('RUN_LEN               =', RUN_LEN                               , file=info_file)\n",
    "    print ('BATCH_SIZE            =', BATCH_SIZE                            , file=info_file)\n",
    "    print ('EARTHQUAKE_WEIGHT     =', EARTHQUAKE_WEIGHT                     , file=info_file)\n",
    "    print ('REVERSE_LOG_BIAS      =', REVERSE_LOG_BIAS                      , file=info_file)\n",
    "    print (' '                                                              , file=info_file)\n",
    "    print ('TRAIN_SHAPE           =', earthquakes_dataset_train.data.shape  , file=info_file)\n",
    "    print ('TEST__SHAPE           =', earthquakes_dataset_test .data.shape  , file=info_file)\n",
    "    print (' '                                                              , file=info_file)\n",
    "    print ('EMB_SIZE              =', EMB_SIZE                              , file=info_file)\n",
    "    print ('HID_SIZE              =', HID_SIZE                              , file=info_file)\n",
    "    \n",
    "    \n",
    "#         print ('', , file=info_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_cell.eval()\n",
    "\n",
    "if SAVE_INFO:\n",
    "    info_file = open (EXPERIMENT_DIR + 'INFO.txt', 'w')\n",
    "else:\n",
    "    info_file = None\n",
    "\n",
    "if SAVE_INFO:\n",
    "    print_INFO_to_file(info_file)\n",
    "    \n",
    "ROC_AUC, AVG_prec = check_quality (RNN_cell,\n",
    "                                   DEVICE,\n",
    "                                   earthquakes_dataloader_test,\n",
    "                                   hid_state,\n",
    "                                   n_dots=251,\n",
    "                                   info_file=info_file)\n",
    "\n",
    "if SAVE_INFO:\n",
    "    info_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_network_CE (network, \n",
    "#                       device,\n",
    "#                       dataloader_train,\n",
    "#                       dataloader_test,\n",
    "#                       epochs=200,\n",
    "#                       learning_rate=0.1,\n",
    "#                       earthquake_weight=1.):\n",
    "    \n",
    "#     if (SAVE_INFO == True):\n",
    "#         epochs_file = open (EXPERIMENT_DIR + 'Epochs_info.txt', 'w')\n",
    "    \n",
    "#     loss_acc  = []\n",
    "#     test_acc  = []\n",
    "#     test_prec = []\n",
    "#     test_rec  = []\n",
    "#     net = network.to(device)\n",
    "\n",
    "#     weights = torch.tensor([1., earthquake_weight], dtype = torch.float).to(device)\n",
    "#     criterion = nn.CrossEntropyLoss(weights)\n",
    "    \n",
    "#     optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, weight_decay=0.0001, momentum=0.9)\n",
    "\n",
    "#     for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
    "#         if epoch == epochs/2:\n",
    "#             optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate/10, weight_decay=0.0001, momentum=0.9)\n",
    "#             print ('Changed learning rate to ', learning_rate/10)\n",
    "#         elif epoch == epochs*3/4:\n",
    "#             optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate/100, weight_decay=0.0001, momentum=0.9) \n",
    "#             print ('Changed learning rate to ', learning_rate/100)\n",
    "            \n",
    "#         net = net.train()        \n",
    "#         epoch_accuracy = 0.0\n",
    "#         epoch_elems = 0\n",
    "#         for data in dataloader_train:\n",
    "            \n",
    "#             inputs = data[0].to(device)\n",
    "#             labels = data[1].to(device)\n",
    "# #             print ('inputs_shape = ', inputs.shape)\n",
    "# #             print ('labels_shape = ', labels.shape)\n",
    "\n",
    "#             # zero the parameter gradients\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # forward + backward + optimize\n",
    "#             outputs = net(inputs)\n",
    "# #             print ('outputs : ', outputs.shape, outputs.dtype)\n",
    "# #             print ('labels  : ', labels.shape , labels.dtype)\n",
    "# #             outputs = torch.cat ((1-outputs, outputs), dim=1)\n",
    "# #             print ('outputs ', outputs.shape, '   [', outputs[1, 0, 12, 12], outputs[1, 1, 12, 12], ']')\n",
    "#             loss = criterion(outputs, labels.squeeze(1).long())\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "# #             print (loss)\n",
    "#             epoch_elems    += labels.shape[0]\n",
    "#             epoch_accuracy += loss.item()*labels.shape[0]\n",
    "\n",
    "#         epoch_accuracy /= epoch_elems\n",
    "#         loss_acc.append(epoch_accuracy)\n",
    "        \n",
    "        \n",
    "# #         calculating test accuracy, precision and recall\n",
    "#         epoch_accuracy  = 0.0\n",
    "#         epoch_precision = 0.0\n",
    "#         epoch_recall    = 0.0\n",
    "#         epoch_elems = 0\n",
    "#         for data in dataloader_test:\n",
    "#             inputs = data[0].to(device)\n",
    "#             labels = data[1].to(device)   \n",
    "#             outputs = net(inputs)\n",
    "# #             find_mistake(outputs)\n",
    "#             accuracy = my_accuracy (outputs[:, 1, :, :].unsqueeze(1), labels, 0.5)\n",
    "#             precision, recall = my_precision_recall (outputs[:, 1, :, :].unsqueeze(1), labels, 0.5)\n",
    "# #             accuracy2 = my_accuracy (outputs, labels, 1.0)\n",
    "            \n",
    "#             epoch_elems     += labels.shape[0]\n",
    "#             epoch_accuracy  += accuracy.item()  * labels.shape[0]\n",
    "#             epoch_precision += precision.item() * labels.shape[0]\n",
    "#             epoch_recall    += recall.item()    * labels.shape[0]\n",
    "\n",
    "# #         epoch_accuracy /= epoch_elems\n",
    "#         test_acc .append (epoch_accuracy  / epoch_elems)\n",
    "#         test_prec.append (epoch_precision / epoch_elems)\n",
    "#         test_rec .append (epoch_recall    / epoch_elems)\n",
    "        \n",
    "        \n",
    "#         print('Ep :', epoch,\n",
    "#               'loss_tr :' , round (loss_acc [-1], 7),\n",
    "#               'acc_ts :'  , round (test_acc [-1], 4),\n",
    "#               'prec_ts :' , round (test_prec[-1], 4),\n",
    "#               'rec_ts :'  , round (test_rec [-1], 4))\n",
    "        \n",
    "#         if (SAVE_INFO == True):\n",
    "#             print('Ep :', epoch,\n",
    "#                   'loss_tr :' , round (loss_acc [-1], 7),\n",
    "#                   'acc_ts :'  , round (test_acc [-1], 4),\n",
    "#                   'prec_ts :' , round (test_prec[-1], 4),\n",
    "#                   'rec_ts :'  , round (test_rec [-1], 4)\n",
    "#                   , file=epochs_file)\n",
    "\n",
    "\n",
    "#     print('Finished Training')\n",
    "    \n",
    "# #     plt.plot(train_acc, label='Train')\n",
    "#     plt.plot(loss_acc , label='Loss')\n",
    "#     plt.legend()\n",
    "#     if (SAVE_INFO == True):\n",
    "#         plt.savefig(EXPERIMENT_DIR + 'Loss_train.png', format='png', dpi=100)\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.plot(test_acc , label='Test Accuracy')\n",
    "#     plt.legend()\n",
    "#     if (SAVE_INFO == True):\n",
    "#         plt.savefig(EXPERIMENT_DIR + 'Accuracy_test.png', format='png', dpi=100)\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.plot(test_prec, label='Test Precision')\n",
    "#     plt.legend()\n",
    "#     if (SAVE_INFO == True):\n",
    "#         plt.savefig(EXPERIMENT_DIR + 'Precision_test.png', format='png', dpi=100)\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.plot(test_rec , label='Test Recall')\n",
    "#     plt.legend()\n",
    "#     if (SAVE_INFO == True):\n",
    "#         plt.savefig(EXPERIMENT_DIR + 'Recall_test.png', format='png', dpi=100)\n",
    "#     plt.show()\n",
    "    \n",
    "#     if (SAVE_INFO == True):\n",
    "#         epochs_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEARNING_RATE = 0.1\n",
    "# N_EPOCHS = 200\n",
    "# EARTHQUAKE_WEIGHT = 10000.\n",
    "\n",
    "# earthquake_network = ConvNetwork_CE ()\n",
    "# train_network_CE  (earthquake_network,\n",
    "#                    torch.device(DEVICE),\n",
    "#                    earthquakes_dataloader_train,\n",
    "#                    earthquakes_dataloader_test,\n",
    "#                    epochs=N_EPOCHS,\n",
    "#                    learning_rate=LEARNING_RATE,\n",
    "#                    earthquake_weight=EARTHQUAKE_WEIGHT\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_ROCinfo (model, dataLoader, device, alpha=0.5, n_dots=101):\n",
    "#     model = model.to(device)\n",
    "    \n",
    "    \n",
    "#     threshold_massive = np.linspace (0, n_dots-1, n_dots, dtype=int)\n",
    "#     TP_massive = np.zeros (n_dots)\n",
    "#     FP_massive = np.zeros (n_dots)\n",
    "#     FN_massive = np.zeros (n_dots)\n",
    "#     TN_massive = np.zeros (n_dots)\n",
    "    \n",
    "#     for data in dataLoader:\n",
    "#         inputs = data[0].to(device)\n",
    "#         labels = data[1].to(device)\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         for threshold in threshold_massive:\n",
    "#             prediction = (outputs[:, 1, :, :].unsqueeze(1))>(threshold/n_dots)\n",
    "#             TP_massive[threshold] += torch.sum (prediction       * labels      ).float()\n",
    "#             FP_massive[threshold] += torch.sum (prediction       * (1 - labels)).float()\n",
    "#             FN_massive[threshold] += torch.sum ((1 - prediction) * labels      ).float()\n",
    "#             TN_massive[threshold] += torch.sum ((1 - prediction) * (1 - labels)).float()\n",
    "            \n",
    "#     threshold_massive = threshold_massive / (n_dots-1)\n",
    "#     precision_massive = TP_massive / (TP_massive + FP_massive)\n",
    "#     TPR_massive       = TP_massive / (TP_massive + FN_massive)\n",
    "#     FPR_massive       = FP_massive / (FP_massive + TN_massive)\n",
    "\n",
    "#     sum_events = TP_massive[int(len(TP_massive)/2)] + FP_massive[int(len(FP_massive)/2)] + FN_massive[int(len(FN_massive)/2)] + TN_massive[int(len(TN_massive)/2)] \n",
    "#     print ('TP = ', round(TP_massive[int(len(TP_massive)/2)] / sum_events, 6), '%')\n",
    "#     print ('FP = ', round(FP_massive[int(len(FP_massive)/2)] / sum_events, 6), '%')\n",
    "#     print ('FN = ', round(FN_massive[int(len(FN_massive)/2)] / sum_events, 6), '%')\n",
    "#     print ('TN = ', round(TN_massive[int(len(TN_massive)/2)] / sum_events, 6), '%')\n",
    "    \n",
    "#     if (SAVE_INFO == True):\n",
    "#         print ('TP = ', round(TP_massive[int(len(TP_massive)/2)] / sum_events, 6), '%', file=INFO_FILE)\n",
    "#         print ('FP = ', round(FP_massive[int(len(FP_massive)/2)] / sum_events, 6), '%', file=INFO_FILE)\n",
    "#         print ('FN = ', round(FN_massive[int(len(FN_massive)/2)] / sum_events, 6), '%', file=INFO_FILE)\n",
    "#         print ('TN = ', round(TN_massive[int(len(TN_massive)/2)] / sum_events, 6), '%', file=INFO_FILE)\n",
    "    \n",
    "#     # plot 1 precision\n",
    "#     fig1 = plt.figure(figsize=(10, 6))\n",
    "\n",
    "#     axes = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "#     axes.plot(threshold_massive, precision_massive, color='green', marker='^')\n",
    "\n",
    "#     axes.set_xlabel('threshold')\n",
    "#     axes.set_ylabel('precision')\n",
    "\n",
    "#     if (SAVE_INFO == True):\n",
    "#         plt.savefig(EXPERIMENT_DIR + 'Precision_from_threshold.png', format='png', dpi=100)\n",
    "#     plt.show()\n",
    "    \n",
    "#     # plot 2 recall\n",
    "#     fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "#     axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "#     axes.plot(threshold_massive, TPR_massive, color='green', marker='^')\n",
    "\n",
    "#     axes.set_xlabel('threshold')\n",
    "#     axes.set_ylabel('recall')\n",
    "    \n",
    "#     if (SAVE_INFO == True):\n",
    "#         plt.savefig(EXPERIMENT_DIR + 'Recall_from_threshold.png', format='png', dpi=100)\n",
    "#     plt.show()\n",
    "    \n",
    "#     # plot 3 ROC-curve\n",
    "#     fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "#     axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "#     axes.plot(FPR_massive, TPR_massive, 'orange', marker = '^')\n",
    "#     axes.plot (range(2), range(2), 'grey', ls='--')\n",
    "\n",
    "#     axes.set_xlabel('FPR')\n",
    "#     axes.set_ylabel('TPR (recall)')\n",
    "#     axes.set_title('ROC-curve')\n",
    "\n",
    "#     if (SAVE_INFO == True):\n",
    "#         plt.savefig(EXPERIMENT_DIR + 'ROC_curve.png', format='png', dpi=100)\n",
    "#     plt.show()\n",
    "    \n",
    "#     del model\n",
    "#     del inputs\n",
    "#     del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvNetwork_MSE (nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(ConvNetwork_CE, self).__init__()\n",
    "        \n",
    "#         self.features = nn.Sequential()\n",
    "        \n",
    "#         self.features.add_module('conv1', conv_block(     OBSERVED_DAYS    , int (OBSERVED_DAYS/2 ), 3))\n",
    "#         self.features.add_module('conv2', conv_block(int (OBSERVED_DAYS/2 ), int (OBSERVED_DAYS/4 ), 3))\n",
    "#         self.features.add_module('conv3', conv_block(int (OBSERVED_DAYS/4 ), int (OBSERVED_DAYS/8 ), 3))\n",
    "#         self.features.add_module('conv4', conv_block(int (OBSERVED_DAYS/8 ), int (OBSERVED_DAYS/16), 3))\n",
    "#         self.features.add_module('conv5', conv_block(int (OBSERVED_DAYS/16),                      1, 3))\n",
    "        \n",
    "#         # might be a good idea to add an extra full connected layer\n",
    "        \n",
    "#     def forward(self, x):\n",
    "# #         print ('input  : ', x.shape)\n",
    "#         x = self.features(x)\n",
    "# #         print ('output : ', x.shape)\n",
    "#         return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_network_MSE(network, \n",
    "#                       device,\n",
    "#                       dataloader_train,\n",
    "#                       dataloader_test,\n",
    "#                       epochs=164,\n",
    "#                       learning_rate=0.1):\n",
    "    \n",
    "#     train_acc = []\n",
    "#     test_acc  = []\n",
    "#     net = network.to(device)\n",
    "\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, weight_decay=0.0001, momentum=0.9)\n",
    "\n",
    "    \n",
    "#     for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
    "#         if epoch == epochs/2:\n",
    "#             optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate/10, weight_decay=0.0001, momentum=0.9) \n",
    "#         elif epoch == epochs*3/4:\n",
    "#             optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate/100, weight_decay=0.0001, momentum=0.9) \n",
    "        \n",
    "#         net = net.train()        \n",
    "#         epoch_accuracy = 0.0\n",
    "#         epoch_elems = 0\n",
    "#         for data in dataloader_train:\n",
    "            \n",
    "#             inputs = data[0].to(device)\n",
    "#             labels = data[1].to(device)\n",
    "# #             print ('inputs_shape = ', inputs.shape)\n",
    "# #             print ('labels_shape = ', labels.shape)\n",
    "\n",
    "#             # zero the parameter gradients\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # forward + backward + optimize\n",
    "#             outputs = net(inputs)\n",
    "# #             print ('outputs : ', outputs.dtype)\n",
    "# #             print ('labels  : ', labels.dtype)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "# #             print (loss)\n",
    "#             epoch_elems    += labels.shape[0]\n",
    "#             epoch_accuracy += loss.item()*labels.shape[0]\n",
    "\n",
    "#         epoch_accuracy /= epoch_elems\n",
    "#         train_acc.append(epoch_accuracy)\n",
    "        \n",
    "        \n",
    "#         # calculating test accuracy\n",
    "#         epoch_accuracy = 0.0\n",
    "#         epoch_elems = 0\n",
    "#         for data in dataloader_test:\n",
    "#             inputs = data[0].to(device)\n",
    "#             labels = data[1].to(device)   \n",
    "#             outputs = net(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             epoch_elems    += labels.shape[0]\n",
    "#             epoch_accuracy += loss.item()*labels.shape[0]\n",
    "\n",
    "#         epoch_accuracy /= epoch_elems\n",
    "#         test_acc.append(epoch_accuracy)\n",
    "        \n",
    "        \n",
    "#         print('Epoch : ', epoch, 'acc_train : ', round (train_acc[-1], 4), 'acc_test : ', round (test_acc[-1], 4))\n",
    "\n",
    "#     print('Finished Training')\n",
    "    \n",
    "#     plt.plot(train_acc, label='Train')\n",
    "#     plt.plot(test_acc , label='Test' )\n",
    "#     plt.legend()\n",
    "#     plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earthquake_netowrk = ConvNetwork_MSE ()\n",
    "# train_network_MSE (earthquake_netowrk,\n",
    "#                    torch.device(DEVICE),\n",
    "#                    earthquakes_dataloader_train,\n",
    "#                    earthquakes_dataloader_test,\n",
    "#                    epochs=200,\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_mistake (x):\n",
    "#     assert (torch.sum((x < 0.0) + (x > 1.0)) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# print ('input  : ', input.shape, input.dtype)\n",
    "# print (input)\n",
    "# print (torch.sum (input, dim = 0))\n",
    "# print ('target : ', target.shape, target.dtype)\n",
    "# print (target)\n",
    "# output = loss(input, target)\n",
    "# output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# class someDataset (Dataset):\n",
    "#     def __init__(self):\n",
    "#         self.data = torch.ones ([100, 32, 10, 10])\n",
    "#         self.labels = torch.ones ([100, 1, 10, 10])\n",
    "#         self.len  = self.data.shape[0]\n",
    "        \n",
    "#         print (self.data.shape)\n",
    "#         print (self.labels.shape)\n",
    "        \n",
    "#     def __len__ (self):\n",
    "#         return self.len\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         print ('data_shape = ', self.data[idx].shape)\n",
    "#         print ('result_shape = ', self.labels[idx].shape)\n",
    "#         return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some_dataset = someDataset()\n",
    "# dataloader = DataLoader (some_dataset,\n",
    "#                          batch_size=32,\n",
    "#                          shuffle=True,\n",
    "#                          num_workers=1,\n",
    "#                          )\n",
    "\n",
    "# for i, batch in enumerate(dataloader, 0):\n",
    "#     data = batch[0]\n",
    "#     print (i, 'data ', data.shape)\n",
    "#     labels = batch[1]\n",
    "#     print (i, 'labels ', labels.shape)\n",
    "    \n",
    "\n",
    "\n",
    "# # eartquakes_dataloader_train = DataLoader(earthquakes_dataset_train,\n",
    "# #                                          batch_size=33,\n",
    "# #                                          shuffle=True,\n",
    "# #                                          num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
