{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "import datetime as dt\n",
    "import random\n",
    "\n",
    "# Results presentation\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NN related stuff\n",
    "import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_INFO = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SAVE_INFO == True):\n",
    "    DATA_DIR = 'Data/'\n",
    "    EXPERIMENT_DIR = DATA_DIR + 'Experiment_conv/'\n",
    "    os.makedirs(EXPERIMENT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT_BORDER = 0\n",
    "RIGHT_BORDER = 2000\n",
    "DOWN_BORDER = 0\n",
    "UP_BORDER = 2500\n",
    "\n",
    "N_CELLS_HOR = 200\n",
    "N_CELLS_VER = 250\n",
    "\n",
    "R_CIRCLE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "celled_data = torch.load(\"Data/celled_data_\" +\n",
    "                         str(N_CELLS_HOR) + \"x\" +\n",
    "                         str(N_CELLS_VER))\n",
    "\n",
    "celled_data_RTL = torch.load (\"Data/RTL_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9591, 1, 200, 250])\n",
      "torch.Size([9591, 9, 200, 250])\n"
     ]
    }
   ],
   "source": [
    "print (celled_data    .shape)\n",
    "print (celled_data_RTL.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_one_day_image (tensor, day):\n",
    "    plt.imshow (tensor[day].squeeze(0), cmap=plt.cm.afmhot_r)\n",
    "    plt.colorbar()\n",
    "\n",
    "def show_one_day_image_RTL (tensor):\n",
    "    plt.imshow (tensor, cmap=plt.cm.afmhot_r)\n",
    "    plt.colorbar()\n",
    "    plt.show\n",
    "    \n",
    "def show_one_day_quakes (tensor, day):\n",
    "    state = tensor[day].squeeze(0)\n",
    "    print (state.shape)\n",
    "    X = []\n",
    "    Y = []\n",
    "    M = []\n",
    "    for i in range(state.shape[0]):\n",
    "        for j in range(state.shape[1]):\n",
    "            if (state[i][j] != 0):\n",
    "                X.append(i)\n",
    "                Y.append(j)\n",
    "                M.append(state[i][j].item())\n",
    "    print (X)\n",
    "    print (Y)\n",
    "    print (M)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 10))\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    plt.axis([0, state.shape[0], 0, state.shape[1]])\n",
    "    axes.scatter(X, Y, s=500, c=M, marker='.', cmap=plt.cm.Reds)\n",
    "#     plt.colorbar()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Распечатаем сколько землетрясений и с какой амплитудой случалось"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_accuracy (input, target, threshold):\n",
    "    true = ((input>threshold) == target)\n",
    "    sum = torch.sum (true.float())\n",
    "    return sum/input.shape[0]/input.shape[1]/input.shape[2]/input.shape[3]\n",
    "\n",
    "def my_precision (input, target, threshold):\n",
    "    TP = torch.sum (((input>threshold) * target      ).float())\n",
    "    FP = torch.sum (((input>threshold) * (1 - target)).float())\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "def my_recall (input, target, threshold):\n",
    "    TP = torch.sum ((     (input>threshold)  * target).float())\n",
    "    FN = torch.sum (((1 - (input>threshold)) * target).float())\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def my_precision_recall (input, target, threshold):\n",
    "    TP = torch.sum ((     (input>threshold)  * target      ).float())\n",
    "    FP = torch.sum ((     (input>threshold)  * (1 - target)).float())\n",
    "    FN = torch.sum (((1 - (input>threshold)) * target      ).float())\n",
    "#     print ('TP = ', TP.item(), 'FP = ', FP.item(), 'FN = ', FN.item(), 'N = ', input.shape[0])\n",
    "    return TP / (TP + FP), TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим датасет\n",
    "#### (Может не влезть в оперативку (надо ~ 12Gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVED_DAYS = 20     # ~2 months\n",
    "DAYS_TO_PREDICT_AFTER  = 10\n",
    "DAYS_TO_PREDICT_BEFORE = 50\n",
    "TESTING_DAYS = 1000\n",
    "\n",
    "HEAVY_QUAKE_THRES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9591, 1, 200, 250])\n"
     ]
    }
   ],
   "source": [
    "print (celled_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_and_Y_pos_train():\n",
    "    Y_more = np.array(celled_data > HEAVY_QUAKE_THRES)\n",
    "    X_train = np.zeros([0, celled_data_RTL.shape[1]], dtype=float)\n",
    "    Y_train = np.zeros([0])\n",
    "    for t in tqdm (range (OBSERVED_DAYS, celled_data.shape[0]-\n",
    "                                         DAYS_TO_PREDICT_BEFORE - \n",
    "                                         TESTING_DAYS)):\n",
    "#         print (X_train.shape)\n",
    "#         print (Y_train.shape)\n",
    "        for i in range (celled_data.shape[2]):\n",
    "            for j in range (celled_data.shape[3]):\n",
    "                X = np.array(celled_data_RTL[t, :, i, j])\n",
    "                Y = np.sum(Y_more[(t + DAYS_TO_PREDICT_AFTER):\n",
    "                                  (t + DAYS_TO_PREDICT_BEFORE), 0, i, j]) > 0\n",
    "                if (Y > 0):\n",
    "#                     print (\"Adding\", t, i, j)\n",
    "                    X_train = np.concatenate((X_train, np.expand_dims(X, axis=0)), axis=0)\n",
    "                    Y_train = np.concatenate((Y_train, np.expand_dims(Y, axis=0)), axis=0)\n",
    "    return X_train, Y_train\n",
    "\n",
    "def create_X_and_Y_neg_train():\n",
    "    Y_more = np.array(celled_data > HEAVY_QUAKE_THRES)\n",
    "    X_train = np.zeros([0, celled_data_RTL.shape[1]], dtype=float)\n",
    "    Y_train = np.zeros([0])\n",
    "    for t in tqdm (range (OBSERVED_DAYS, celled_data.shape[0]-\n",
    "                                         DAYS_TO_PREDICT_BEFORE - \n",
    "                                         TESTING_DAYS)):\n",
    "#         print (X_train.shape)\n",
    "#         print (Y_train.shape)\n",
    "        for i in range (celled_data.shape[2]):\n",
    "            for j in range (celled_data.shape[3]):\n",
    "                X = np.array(celled_data_RTL[t, :, i, j])\n",
    "                Y = np.sum(Y_more[(t + DAYS_TO_PREDICT_AFTER):\n",
    "                                  (t + DAYS_TO_PREDICT_BEFORE), 0, i, j]) > 0\n",
    "                if (Y == 0 and random.randint(1, 500) == 500):\n",
    "#                     print (\"Adding\", t, i, j, Y)\n",
    "                    X_train = np.concatenate((X_train, np.expand_dims(X, axis=0)), axis=0)\n",
    "                    Y_train = np.concatenate((Y_train, np.expand_dims(Y, axis=0)), axis=0)\n",
    "    return X_train, Y_train\n",
    "\n",
    "def create_X_and_Y_pos_test():\n",
    "    Y_more = np.array(celled_data > HEAVY_QUAKE_THRES)\n",
    "    X_train = np.zeros([0, celled_data_RTL.shape[1]], dtype=float)\n",
    "    Y_train = np.zeros([0])\n",
    "    for t in tqdm (range (celled_data.shape[0]-\n",
    "                          DAYS_TO_PREDICT_BEFORE - \n",
    "                          TESTING_DAYS,\n",
    "                          celled_data.shape[0]-\n",
    "                          DAYS_TO_PREDICT_BEFORE)):\n",
    "#         print (X_train.shape)\n",
    "#         print (Y_train.shape)\n",
    "        for i in range (celled_data.shape[2]):\n",
    "            for j in range (celled_data.shape[3]):\n",
    "                X = np.array(celled_data_RTL[t, :, i, j])\n",
    "                Y = np.sum(Y_more[(t + DAYS_TO_PREDICT_AFTER):\n",
    "                                  (t + DAYS_TO_PREDICT_BEFORE), 0, i, j]) > 0\n",
    "                if (Y > 0):\n",
    "#                     print (\"Adding\", t, i, j)\n",
    "                    X_train = np.concatenate((X_train, np.expand_dims(X, axis=0)), axis=0)\n",
    "                    Y_train = np.concatenate((Y_train, np.expand_dims(Y, axis=0)), axis=0)\n",
    "    return X_train, Y_train\n",
    "\n",
    "def create_X_and_Y_neg_test():\n",
    "    Y_more = np.array(celled_data > HEAVY_QUAKE_THRES)\n",
    "    X_train = np.zeros([0, celled_data_RTL.shape[1]], dtype=float)\n",
    "    Y_train = np.zeros([0])\n",
    "    for t in tqdm (range (celled_data.shape[0]-\n",
    "                          DAYS_TO_PREDICT_BEFORE - \n",
    "                          TESTING_DAYS,\n",
    "                          celled_data.shape[0]-\n",
    "                          DAYS_TO_PREDICT_BEFORE)):\n",
    "#         print (X_train.shape)\n",
    "#         print (Y_train.shape)\n",
    "        for i in range (celled_data.shape[2]):\n",
    "            for j in range (celled_data.shape[3]):\n",
    "                X = np.array(celled_data_RTL[t, :, i, j])\n",
    "                Y = np.sum(Y_more[(t + DAYS_TO_PREDICT_AFTER):\n",
    "                                  (t + DAYS_TO_PREDICT_BEFORE), 0, i, j]) > 0\n",
    "                if (Y == 0):\n",
    "#                     print (\"Adding\", t, i, j, Y)\n",
    "                    X_train = np.concatenate((X_train, np.expand_dims(X, axis=0)), axis=0)\n",
    "                    Y_train = np.concatenate((Y_train, np.expand_dims(Y, axis=0)), axis=0)\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e25b6215e74cbaa1e79f4945089ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, Y = create_X_and_Y_neg_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X.shape)\n",
    "print (Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Data/boosting_RTL/M=5/X_neg_test\", X)\n",
    "np.savetxt(\"Data/boosting_RTL/M=5/Y_neg_test\", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.loadtxt(\"Data/boosting_RTL/X_pos\")\n",
    "# Y_train = np.loadtxt(\"Data/boosting_RTL/Y_pos\")\n",
    "# print (X_train.shape)\n",
    "# print (Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_X_and_Y_train():\n",
    "#     X_train = np.zeros([0, OBSERVED_DAYS], dtype=float)\n",
    "#     Y_train = np.zeros([0])\n",
    "#     for t in tqdm (range (OBSERVED_DAYS, celled_data.shape[0] -\n",
    "#                                          TESTING_DAYS -\n",
    "#                                          DAYS_TO_PREDICT_BEFORE)):\n",
    "#         for i in range (celled_data_RTL.shape[2]):\n",
    "#             for j in range (celled_data_RTL.shape[3]):\n",
    "#                 X = celled_data_RTL[t, 0, i, j]\n",
    "#                 Y = np.sum((celled_data[(t + DAYS_TO_PREDICT_AFTER):\n",
    "#                                         (t + DAYS_TO_PREDICT_BEFORE), 0, i, j] >\n",
    "#                             HEAVY_QUAKE_THRES)) > 0\n",
    "#                 if (np.sum(X) > 0):\n",
    "#                     X_train = np.concatenate((X_train, np.expand_dims(X, axis=0)), axis=0)\n",
    "#                     Y_train = np.concatenate((Y_train, np.expand_dims(Y, axis=0)), axis=0)\n",
    "\n",
    "#     #             print (X_train.shape)\n",
    "#     #             print (Y_train.shape)\n",
    "#     return X_train, Y_train\n",
    "\n",
    "# def create_X_and_Y_test():\n",
    "#     X_test = np.zeros([0, OBSERVED_DAYS], dtype=float)\n",
    "#     Y_test = np.zeros([0])\n",
    "#     for t in tqdm (range (celled_data.shape[0] -\n",
    "#                           TESTING_DAYS +\n",
    "#                           OBSERVED_DAYS,\n",
    "#                           celled_data.shape[0] -\n",
    "#                           DAYS_TO_PREDICT_BEFORE)):\n",
    "#         for i in range (celled_data.shape[2]):\n",
    "#             for j in range (celled_data.shape[3]):\n",
    "#                 X = celled_data_RTL[t, 0, i, j]\n",
    "#                 Y = np.sum((celled_data[(t + DAYS_TO_PREDICT_AFTER):\n",
    "#                                         (t + DAYS_TO_PREDICT_BEFORE), 0, i, j] >\n",
    "#                             HEAVY_QUAKE_THRES)) > 0\n",
    "#                 if (np.sum(X) > 0):\n",
    "#                     X_test = np.concatenate((X_test, np.expand_dims(X, axis=0)), axis=0)\n",
    "#                     Y_test = np.concatenate((Y_test, np.expand_dims(Y, axis=0)), axis=0)\n",
    "\n",
    "#     #             print (X_test.shape)\n",
    "#     #             print (Y_test.shape)\n",
    "#     return X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train = create_X_and_Y_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"Data/train_boosting/X\", X_train)\n",
    "# np.savetxt(\"Data/train_boosting/Y\", Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test , Y_test  = create_X_and_Y_test ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"Data/test_boosting/X\" , X_test)\n",
    "# np.savetxt(\"Data/test_boosting/Y\" , Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.loadtxt(\"Data/train_boosting/X\")\n",
    "# Y_train = np.loadtxt(\"Data/train_boosting/Y\")\n",
    "# print (X_train.shape)\n",
    "# print (Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test  = np.loadtxt(\"Data/test_boosting/X\")\n",
    "# Y_test  = np.loadtxt(\"Data/test_boosting/Y\")\n",
    "# print (X_test .shape)\n",
    "# print (Y_test .shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.zeros([(celled_data.shape[0] -\n",
    "#                      OBSERVED_DAYS -\n",
    "#                      TESTING_DAYS -\n",
    "#                      DAYS_TO_PREDICT_BEFORE) *\n",
    "#                     N_CELLS_HOR *\n",
    "#                     N_CELLS_VER,\n",
    "#                     OBSERVED_DAYS], \n",
    "#                    dtype=float)\n",
    "# Y_train = np.zeros([(celled_data.shape[0] -\n",
    "#                      OBSERVED_DAYS -\n",
    "#                      TESTING_DAYS -\n",
    "#                      DAYS_TO_PREDICT_BEFORE) *\n",
    "#                     N_CELLS_HOR *\n",
    "#                     N_CELLS_VER,\n",
    "#                     OBSERVED_DAYS])\n",
    "\n",
    "# for t in tqdm (range (OBSERVED_DAYS, celled_data.shape[0] -\n",
    "#                                      TESTING_DAYS -\n",
    "#                                      DAYS_TO_PREDICT_BEFORE)):\n",
    "#     for i in range (celled_data.shape[2]):\n",
    "#         for j in range (celled_data.shape[3]):\n",
    "#             X_train[t * N_CELLS_HOR * N_CELLS_VER + i * N_CELLS_VER + j] = \\\n",
    "#             celled_data[(t - OBSERVED_DAYS):t, 0, i, j]\n",
    "            \n",
    "#             Y_train[t * N_CELLS_HOR * N_CELLS_VER + i * N_CELLS_VER + j] = \\\n",
    "#             np.sum(celled_data[(t + DAYS_TO_PREDICT_AFTER):\n",
    "#                                (t + DAYS_TO_PREDICT_BEFORE), 0, i, j] >\n",
    "#                    HEAVY_QUAKE_THRES) > 0\n",
    "# #             X_train = np.concatenate((X_train, np.expand_dims(X, axis=0)), axis=0)\n",
    "# #             Y_train = np.concatenate((Y_train, np.expand_dims(Y, axis=0)), axis=0)\n",
    "            \n",
    "# #             print (X_train.shape)\n",
    "# #             print (Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_neg = np.loadtxt(\"Data/boosting_RTL/M=5/X_neg_train\")\n",
    "X_pos = np.loadtxt(\"Data/boosting_RTL/M=5/X_pos_train\")\n",
    "Y_neg = np.loadtxt(\"Data/boosting_RTL/M=5/Y_neg_train\")\n",
    "Y_pos = np.loadtxt(\"Data/boosting_RTL/M=5/Y_pos_train\")\n",
    "print (X_neg.shape, X_neg.dtype)\n",
    "print (X_pos.shape, X_pos.dtype)\n",
    "print (Y_neg.shape, Y_neg.dtype)\n",
    "print (Y_pos.shape, Y_pos.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = np.concatenate((X_neg, X_pos), axis=0)\n",
    "Y_all = np.concatenate((Y_neg, Y_pos), axis=0)\n",
    "print (X_all.shape)\n",
    "print (Y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_all, Y_all, test_size=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_neg_test = np.loadtxt(\"Data/boosting_RTL/M=5/X_neg_test\")\n",
    "X_pos_test = np.loadtxt(\"Data/boosting_RTL/M=5/X_pos_test\")\n",
    "Y_neg_test = np.loadtxt(\"Data/boosting_RTL/M=5/Y_neg_test\")\n",
    "Y_pos_test = np.loadtxt(\"Data/boosting_RTL/M=5/Y_pos_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test  = np.concatenate((X_neg_test, X_pos_test), axis=0)\n",
    "Y_test = np.concatenate((Y_neg_test, Y_pos_test), axis=0)\n",
    "print (X_all.shape)\n",
    "print (Y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.shape)\n",
    "print (Y_train.shape)\n",
    "print (X_test.shape)\n",
    "print (Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (Y_test.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим саму сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix (X_train, label=Y_train)\n",
    "dtest  = xgb.DMatrix (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_clf = xgb.XGBClassifier(objective='binary:logistic', n_jobs=4)\n",
    "boost_clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = boost_clf.predict(X_test)\n",
    "\n",
    "ROC_AUC_score = roc_auc_score(Y_test, prediction)\n",
    "Avg_precision = average_precision_score(Y_test, prediction)\n",
    "\n",
    "print (\"ROC_AUC = \", ROC_AUC_score, \"Avg_precision = \", Avg_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_clf = xgb.XGBClassifier(objective='binary:logistic', n_jobs=16)\n",
    "\n",
    "param_grid = {'n_jobs'      :[16],\n",
    "              'objective'   :['binary:logistic'],\n",
    "              'max_depth'   :[1, 3, 5, 11],\n",
    "              'n_estimators':[2, 10, 20],\n",
    "              'booster'     :['gbtree', 'gblinear']\n",
    "             } # might be useful to try to change some other params\n",
    "grid_cv = GridSearchCV (boost_clf, param_grid, scoring=make_scorer(average_precision_score), cv=5)\n",
    "\n",
    "grid_cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best params:')\n",
    "print (grid_cv.best_params_)\n",
    "\n",
    "best_boost_clf = xgb.XGBClassifier(n_jobs      =grid_cv.best_params_['n_jobs'],\n",
    "                                   objective   =grid_cv.best_params_['objective'],\n",
    "                                   max_depth   =grid_cv.best_params_['max_depth'],\n",
    "                                   n_estimators=grid_cv.best_params_['n_estimators'],\n",
    "                                   booster     =grid_cv.best_params_['booster'])\n",
    "\n",
    "best_boost_clf.fit (X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = best_boost_clf.predict(X_test)\n",
    "\n",
    "ROC_AUC_score = roc_auc_score(Y_test, prediction)\n",
    "Avg_precision = average_precision_score(Y_test, prediction)\n",
    "\n",
    "print (\"ROC_AUC = \", ROC_AUC_score, \"Avg_precision = \", Avg_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (4, 21, 2):\n",
    "    boost_clf = xgb.XGBClassifier(n_jobs      = 4,\n",
    "                                  objective   = 'binary:logistic',\n",
    "                                  max_depth   = 10,\n",
    "                                  n_estimators= 20,\n",
    "                                  booster     = 'gbtree')\n",
    "    boost_clf.fit (X_train[:, 0:i], Y_train)\n",
    "    \n",
    "    prediction = boost_clf.predict(X_test[:, 0:i])\n",
    "    ROC_AUC_score = roc_auc_score          (Y_test, prediction)\n",
    "    Avg_precision = average_precision_score(Y_test, prediction)\n",
    "    print (\"i = \", i, \"ROC_AUC = \", ROC_AUC_score, \"Avg_precision = \", Avg_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_ROCinfo (model, dataLoader, device, alpha=0.5, n_dots=101):\n",
    "#     model = model.to(device)\n",
    "    \n",
    "    \n",
    "#     threshold_massive = np.linspace (0, n_dots-1, n_dots, dtype=int)\n",
    "#     TP_massive = np.zeros (n_dots)\n",
    "#     FP_massive = np.zeros (n_dots)\n",
    "#     FN_massive = np.zeros (n_dots)\n",
    "#     TN_massive = np.zeros (n_dots)\n",
    "    \n",
    "#     for data in dataLoader:\n",
    "#         inputs = data[0].to(device)\n",
    "#         labels = data[1].to(device)\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         for threshold in threshold_massive:\n",
    "#             prediction = (outputs[:, 1, :, :].unsqueeze(1))>(threshold/n_dots)\n",
    "#             TP_massive[threshold] += torch.sum (prediction       * labels      ).float()\n",
    "#             FP_massive[threshold] += torch.sum (prediction       * (1 - labels)).float()\n",
    "#             FN_massive[threshold] += torch.sum ((1 - prediction) * labels      ).float()\n",
    "#             TN_massive[threshold] += torch.sum ((1 - prediction) * (1 - labels)).float()\n",
    "            \n",
    "#     threshold_massive = threshold_massive / (n_dots-1)\n",
    "#     precision_massive = TP_massive / (TP_massive + FP_massive)\n",
    "#     TPR_massive       = TP_massive / (TP_massive + FN_massive)\n",
    "#     FPR_massive       = FP_massive / (FP_massive + TN_massive)\n",
    "\n",
    "#     sum_events = TP_massive[int(len(TP_massive)/2)] + FP_massive[int(len(FP_massive)/2)] + FN_massive[int(len(FN_massive)/2)] + TN_massive[int(len(TN_massive)/2)] \n",
    "#     print ('TP = ', round(TP_massive[int(len(TP_massive)/2)] / sum_events, 6), '%')\n",
    "#     print ('FP = ', round(FP_massive[int(len(FP_massive)/2)] / sum_events, 6), '%')\n",
    "#     print ('FN = ', round(FN_massive[int(len(FN_massive)/2)] / sum_events, 6), '%')\n",
    "#     print ('TN = ', round(TN_massive[int(len(TN_massive)/2)] / sum_events, 6), '%')\n",
    "    \n",
    "#     if (SAVE_INFO == True):\n",
    "#         print ('TP = ', round(TP_massive[int(len(TP_massive)/2)] / sum_events, 6), '%', file=INFO_FILE)\n",
    "#         print ('FP = ', round(FP_massive[int(len(FP_massive)/2)] / sum_events, 6), '%', file=INFO_FILE)\n",
    "#         print ('FN = ', round(FN_massive[int(len(FN_massive)/2)] / sum_events, 6), '%', file=INFO_FILE)\n",
    "#         print ('TN = ', round(TN_massive[int(len(TN_massive)/2)] / sum_events, 6), '%', file=INFO_FILE)\n",
    "    \n",
    "#     # plot 1 precision\n",
    "#     fig1 = plt.figure(figsize=(10, 6))\n",
    "\n",
    "#     axes = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "#     axes.plot(threshold_massive, precision_massive, color='green', marker='^')\n",
    "\n",
    "#     axes.set_xlabel('threshold')\n",
    "#     axes.set_ylabel('precision')\n",
    "\n",
    "#     if (SAVE_INFO == True):\n",
    "#         plt.savefig(EXPERIMENT_DIR + 'Precision_from_threshold.png', format='png', dpi=100)\n",
    "#     plt.show()\n",
    "    \n",
    "#     # plot 2 recall\n",
    "#     fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "#     axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "#     axes.plot(threshold_massive, TPR_massive, color='green', marker='^')\n",
    "\n",
    "#     axes.set_xlabel('threshold')\n",
    "#     axes.set_ylabel('recall')\n",
    "    \n",
    "#     if (SAVE_INFO == True):\n",
    "#         plt.savefig(EXPERIMENT_DIR + 'Recall_from_threshold.png', format='png', dpi=100)\n",
    "#     plt.show()\n",
    "    \n",
    "#     # plot 3 ROC-curve\n",
    "#     fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "#     axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "#     axes.plot(FPR_massive, TPR_massive, 'orange', marker = '^')\n",
    "#     axes.plot (range(2), range(2), 'grey', ls='--')\n",
    "\n",
    "#     axes.set_xlabel('FPR')\n",
    "#     axes.set_ylabel('TPR (recall)')\n",
    "#     axes.set_title('ROC-curve')\n",
    "\n",
    "#     if (SAVE_INFO == True):\n",
    "#         plt.savefig(EXPERIMENT_DIR + 'ROC_curve.png', format='png', dpi=100)\n",
    "#     plt.show()\n",
    "    \n",
    "#     del model\n",
    "#     del inputs\n",
    "#     del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_INFO_to_file():\n",
    "    if (SAVE_INFO == True):\n",
    "        print ('ORIGIN_LATITUDE        =', ORIGIN_LATITUDE                       , file=INFO_FILE)\n",
    "        print ('ORIGIN_LONGITUDE       =', ORIGIN_LONGITUDE                      , file=INFO_FILE)\n",
    "        print ('LEFT_BORDER            =', LEFT_BORDER                           , file=INFO_FILE)\n",
    "        print ('RIGHT_BORDER           =', RIGHT_BORDER                          , file=INFO_FILE)\n",
    "        print ('DOWN_BORDER            =', DOWN_BORDER                           , file=INFO_FILE)\n",
    "        print ('UP_BORDER              =', UP_BORDER                             , file=INFO_FILE)\n",
    "        print ('N_CELLS_HOR            =', N_CELLS_HOR                           , file=INFO_FILE)\n",
    "        print ('N_CELLS_VER            =', N_CELLS_VER                           , file=INFO_FILE)\n",
    "        print (' '                                                               , file=INFO_FILE)\n",
    "        print ('OBSERVED_DAYS          =', OBSERVED_DAYS                         , file=INFO_FILE)\n",
    "        print ('DAYS_TO_PREDICT        =', DAYS_TO_PREDICT                       , file=INFO_FILE)\n",
    "        print ('STEP                   =', STEP                                  , file=INFO_FILE)\n",
    "        print ('TESTING_DAYS           =', TESTING_DAYS                          , file=INFO_FILE)\n",
    "        print ('HEAVY_QUAKE_THRES      =', HEAVY_QUAKE_THRES                     , file=INFO_FILE)\n",
    "        print ('LEARNING_RATE          =', LEARNING_RATE                         , file=INFO_FILE)\n",
    "        print ('N_EPOCHS               =', N_EPOCHS                              , file=INFO_FILE)\n",
    "        print ('EARTHQUAKE_WEIGHT      =', EARTHQUAKE_WEIGHT                     , file=INFO_FILE)\n",
    "        print ('TRAIN_SHAPE            =', earthquakes_dataset_train.data.shape  , file=INFO_FILE)\n",
    "        print ('TEST__SHAPE            =', earthquakes_dataset_test .result.shape, file=INFO_FILE)\n",
    "        \n",
    "#         print ('', , file=INFO_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SAVE_INFO == True):\n",
    "    INFO_FILE = open (EXPERIMENT_DIR + 'INFO.txt', 'w')\n",
    "    \n",
    "if (SAVE_INFO == True):\n",
    "    print_INFO_to_file()\n",
    "    \n",
    "print_ROCinfo (earthquake_network,\n",
    "               earthquakes_dataloader_test,\n",
    "               DEVICE,\n",
    "               ) \n",
    "\n",
    "if (SAVE_INFO == True):\n",
    "    INFO_FILE.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvNetwork_MSE (nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(ConvNetwork_CE, self).__init__()\n",
    "        \n",
    "#         self.features = nn.Sequential()\n",
    "        \n",
    "#         self.features.add_module('conv1', conv_block(     OBSERVED_DAYS    , int (OBSERVED_DAYS/2 ), 3))\n",
    "#         self.features.add_module('conv2', conv_block(int (OBSERVED_DAYS/2 ), int (OBSERVED_DAYS/4 ), 3))\n",
    "#         self.features.add_module('conv3', conv_block(int (OBSERVED_DAYS/4 ), int (OBSERVED_DAYS/8 ), 3))\n",
    "#         self.features.add_module('conv4', conv_block(int (OBSERVED_DAYS/8 ), int (OBSERVED_DAYS/16), 3))\n",
    "#         self.features.add_module('conv5', conv_block(int (OBSERVED_DAYS/16),                      1, 3))\n",
    "        \n",
    "#         # might be a good idea to add an extra full connected layer\n",
    "        \n",
    "#     def forward(self, x):\n",
    "# #         print ('input  : ', x.shape)\n",
    "#         x = self.features(x)\n",
    "# #         print ('output : ', x.shape)\n",
    "#         return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_network_MSE(network, \n",
    "#                       device,\n",
    "#                       dataloader_train,\n",
    "#                       dataloader_test,\n",
    "#                       epochs=164,\n",
    "#                       learning_rate=0.1):\n",
    "    \n",
    "#     train_acc = []\n",
    "#     test_acc  = []\n",
    "#     net = network.to(device)\n",
    "\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, weight_decay=0.0001, momentum=0.9)\n",
    "\n",
    "    \n",
    "#     for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
    "#         if epoch == epochs/2:\n",
    "#             optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate/10, weight_decay=0.0001, momentum=0.9) \n",
    "#         elif epoch == epochs*3/4:\n",
    "#             optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate/100, weight_decay=0.0001, momentum=0.9) \n",
    "        \n",
    "#         net = net.train()        \n",
    "#         epoch_accuracy = 0.0\n",
    "#         epoch_elems = 0\n",
    "#         for data in dataloader_train:\n",
    "            \n",
    "#             inputs = data[0].to(device)\n",
    "#             labels = data[1].to(device)\n",
    "# #             print ('inputs_shape = ', inputs.shape)\n",
    "# #             print ('labels_shape = ', labels.shape)\n",
    "\n",
    "#             # zero the parameter gradients\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # forward + backward + optimize\n",
    "#             outputs = net(inputs)\n",
    "# #             print ('outputs : ', outputs.dtype)\n",
    "# #             print ('labels  : ', labels.dtype)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "# #             print (loss)\n",
    "#             epoch_elems    += labels.shape[0]\n",
    "#             epoch_accuracy += loss.item()*labels.shape[0]\n",
    "\n",
    "#         epoch_accuracy /= epoch_elems\n",
    "#         train_acc.append(epoch_accuracy)\n",
    "        \n",
    "        \n",
    "#         # calculating test accuracy\n",
    "#         epoch_accuracy = 0.0\n",
    "#         epoch_elems = 0\n",
    "#         for data in dataloader_test:\n",
    "#             inputs = data[0].to(device)\n",
    "#             labels = data[1].to(device)   \n",
    "#             outputs = net(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             epoch_elems    += labels.shape[0]\n",
    "#             epoch_accuracy += loss.item()*labels.shape[0]\n",
    "\n",
    "#         epoch_accuracy /= epoch_elems\n",
    "#         test_acc.append(epoch_accuracy)\n",
    "        \n",
    "        \n",
    "#         print('Epoch : ', epoch, 'acc_train : ', round (train_acc[-1], 4), 'acc_test : ', round (test_acc[-1], 4))\n",
    "\n",
    "#     print('Finished Training')\n",
    "    \n",
    "#     plt.plot(train_acc, label='Train')\n",
    "#     plt.plot(test_acc , label='Test' )\n",
    "#     plt.legend()\n",
    "#     plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earthquake_netowrk = ConvNetwork_MSE ()\n",
    "# train_network_MSE (earthquake_netowrk,\n",
    "#                    torch.device(DEVICE),\n",
    "#                    earthquakes_dataloader_train,\n",
    "#                    earthquakes_dataloader_test,\n",
    "#                    epochs=200,\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_mistake (x):\n",
    "#     assert (torch.sum((x < 0.0) + (x > 1.0)) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# print ('input  : ', input.shape, input.dtype)\n",
    "# print (input)\n",
    "# print (torch.sum (input, dim = 0))\n",
    "# print ('target : ', target.shape, target.dtype)\n",
    "# print (target)\n",
    "# output = loss(input, target)\n",
    "# output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# class someDataset (Dataset):\n",
    "#     def __init__(self):\n",
    "#         self.data = torch.ones ([100, 32, 10, 10])\n",
    "#         self.labels = torch.ones ([100, 1, 10, 10])\n",
    "#         self.len  = self.data.shape[0]\n",
    "        \n",
    "#         print (self.data.shape)\n",
    "#         print (self.labels.shape)\n",
    "        \n",
    "#     def __len__ (self):\n",
    "#         return self.len\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         print ('data_shape = ', self.data[idx].shape)\n",
    "#         print ('result_shape = ', self.labels[idx].shape)\n",
    "#         return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some_dataset = someDataset()\n",
    "# dataloader = DataLoader (some_dataset,\n",
    "#                          batch_size=32,\n",
    "#                          shuffle=True,\n",
    "#                          num_workers=1,\n",
    "#                          )\n",
    "\n",
    "# for i, batch in enumerate(dataloader, 0):\n",
    "#     data = batch[0]\n",
    "#     print (i, 'data ', data.shape)\n",
    "#     labels = batch[1]\n",
    "#     print (i, 'labels ', labels.shape)\n",
    "    \n",
    "\n",
    "\n",
    "# # eartquakes_dataloader_train = DataLoader(earthquakes_dataset_train,\n",
    "# #                                          batch_size=33,\n",
    "# #                                          shuffle=True,\n",
    "# #                                          num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
