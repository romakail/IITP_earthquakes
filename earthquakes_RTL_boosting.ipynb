{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "import datetime as dt\n",
    "import random\n",
    "\n",
    "# Results presentation\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NN related stuff\n",
    "import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_INFO = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SAVE_INFO == True):\n",
    "    DATA_DIR = 'Data/'\n",
    "    EXPERIMENT_DIR = DATA_DIR + 'Experiment_conv/'\n",
    "    os.makedirs(EXPERIMENT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT_BORDER = 0\n",
    "RIGHT_BORDER = 2000\n",
    "DOWN_BORDER = 0\n",
    "UP_BORDER = 2500\n",
    "\n",
    "N_CELLS_HOR = 200\n",
    "N_CELLS_VER = 250\n",
    "\n",
    "R_CIRCLE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "celled_data = torch.load(\"Data/celled_data_\" +\n",
    "                         str(N_CELLS_HOR) + \"x\" +\n",
    "                         str(N_CELLS_VER))\n",
    "\n",
    "celled_data_RTL = torch.load (\"Data/RTL_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9591, 1, 200, 250])\n",
      "torch.Size([9591, 9, 200, 250])\n"
     ]
    }
   ],
   "source": [
    "print (celled_data    .shape)\n",
    "print (celled_data_RTL.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_one_day_image (tensor, day):\n",
    "    plt.imshow (tensor[day].squeeze(0), cmap=plt.cm.afmhot_r)\n",
    "    plt.colorbar()\n",
    "\n",
    "def show_one_day_image_RTL (tensor):\n",
    "    plt.imshow (tensor, cmap=plt.cm.afmhot_r)\n",
    "    plt.colorbar()\n",
    "    plt.show\n",
    "    \n",
    "def show_one_day_quakes (tensor, day):\n",
    "    state = tensor[day].squeeze(0)\n",
    "    print (state.shape)\n",
    "    X = []\n",
    "    Y = []\n",
    "    M = []\n",
    "    for i in range(state.shape[0]):\n",
    "        for j in range(state.shape[1]):\n",
    "            if (state[i][j] != 0):\n",
    "                X.append(i)\n",
    "                Y.append(j)\n",
    "                M.append(state[i][j].item())\n",
    "    print (X)\n",
    "    print (Y)\n",
    "    print (M)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 10))\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    plt.axis([0, state.shape[0], 0, state.shape[1]])\n",
    "    axes.scatter(X, Y, s=500, c=M, marker='.', cmap=plt.cm.Reds)\n",
    "#     plt.colorbar()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Распечатаем сколько землетрясений и с какой амплитудой случалось"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_accuracy (input, target, threshold):\n",
    "    true = ((input>threshold) == target)\n",
    "    sum = torch.sum (true.float())\n",
    "    return sum/input.shape[0]/input.shape[1]/input.shape[2]/input.shape[3]\n",
    "\n",
    "def my_precision (input, target, threshold):\n",
    "    TP = torch.sum (((input>threshold) * target      ).float())\n",
    "    FP = torch.sum (((input>threshold) * (1 - target)).float())\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "def my_recall (input, target, threshold):\n",
    "    TP = torch.sum ((     (input>threshold)  * target).float())\n",
    "    FN = torch.sum (((1 - (input>threshold)) * target).float())\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def my_precision_recall (input, target, threshold):\n",
    "    TP = torch.sum ((     (input>threshold)  * target      ).float())\n",
    "    FP = torch.sum ((     (input>threshold)  * (1 - target)).float())\n",
    "    FN = torch.sum (((1 - (input>threshold)) * target      ).float())\n",
    "#     print ('TP = ', TP.item(), 'FP = ', FP.item(), 'FN = ', FN.item(), 'N = ', input.shape[0])\n",
    "    return TP / (TP + FP), TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим датасет\n",
    "#### (Может не влезть в оперативку (надо ~ 12Gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVED_DAYS = 20     # ~2 months\n",
    "DAYS_TO_PREDICT_AFTER  = 10\n",
    "DAYS_TO_PREDICT_BEFORE = 50\n",
    "TESTING_DAYS = 1000\n",
    "\n",
    "HEAVY_QUAKE_THRES = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9591, 1, 200, 250])\n"
     ]
    }
   ],
   "source": [
    "print (celled_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_and_Y_pos():\n",
    "    Y_more = np.array(celled_data > HEAVY_QUAKE_THRES)\n",
    "    X_train = np.zeros([0, celled_data_RTL.shape[1]], dtype=float)\n",
    "    Y_train = np.zeros([0])\n",
    "    for t in tqdm (range (OBSERVED_DAYS, celled_data.shape[0]-\n",
    "                                         DAYS_TO_PREDICT_BEFORE)):\n",
    "#         print (X_train.shape)\n",
    "#         print (Y_train.shape)\n",
    "        for i in range (celled_data.shape[2]):\n",
    "            for j in range (celled_data.shape[3]):\n",
    "                X = np.array(celled_data_RTL[t, :, i, j])\n",
    "                Y = np.sum(Y_more[(t + DAYS_TO_PREDICT_AFTER):\n",
    "                                  (t + DAYS_TO_PREDICT_BEFORE), 0, i, j]) > 0\n",
    "                if (Y > 0):\n",
    "#                     print (\"Adding\", t, i, j)\n",
    "                    X_train = np.concatenate((X_train, np.expand_dims(X, axis=0)), axis=0)\n",
    "                    Y_train = np.concatenate((Y_train, np.expand_dims(Y, axis=0)), axis=0)\n",
    "    return X_train, Y_train\n",
    "\n",
    "def create_X_and_Y_neg():\n",
    "    Y_more = np.array(celled_data > HEAVY_QUAKE_THRES)\n",
    "    X_train = np.zeros([0, celled_data_RTL.shape[1]], dtype=float)\n",
    "    Y_train = np.zeros([0])\n",
    "    for t in tqdm (range (OBSERVED_DAYS, celled_data.shape[0]-\n",
    "                                         DAYS_TO_PREDICT_BEFORE)):\n",
    "#         print (X_train.shape)\n",
    "#         print (Y_train.shape)\n",
    "        for i in range (celled_data.shape[2]):\n",
    "            for j in range (celled_data.shape[3]):\n",
    "                X = np.array(celled_data_RTL[t, :, i, j])\n",
    "                Y = np.sum(Y_more[(t + DAYS_TO_PREDICT_AFTER):\n",
    "                                  (t + DAYS_TO_PREDICT_BEFORE), 0, i, j]) > 0\n",
    "                if (Y == 0 and random.randint(1, 500) == 500):\n",
    "                    print (\"Adding\", t, i, j, Y)\n",
    "                    X_train = np.concatenate((X_train, np.expand_dims(X, axis=0)), axis=0)\n",
    "                    Y_train = np.concatenate((Y_train, np.expand_dims(Y, axis=0)), axis=0)\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3b37b2595344bdbc94efbb4ff1bd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9521), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 20 2 235 False\n",
      "Adding 20 5 125 False\n",
      "Adding 20 5 242 False\n",
      "Adding 20 7 130 False\n",
      "Adding 20 9 195 False\n",
      "Adding 20 14 166 False\n",
      "Adding 20 14 224 False\n",
      "Adding 20 15 214 False\n",
      "Adding 20 19 173 False\n",
      "Adding 20 23 249 False\n",
      "Adding 20 25 156 False\n",
      "Adding 20 25 164 False\n",
      "Adding 20 32 124 False\n",
      "Adding 20 33 206 False\n",
      "Adding 20 34 243 False\n",
      "Adding 20 35 19 False\n",
      "Adding 20 37 132 False\n",
      "Adding 20 38 204 False\n",
      "Adding 20 39 142 False\n",
      "Adding 20 40 128 False\n",
      "Adding 20 41 141 False\n",
      "Adding 20 42 98 False\n",
      "Adding 20 42 166 False\n",
      "Adding 20 44 64 False\n",
      "Adding 20 45 36 False\n",
      "Adding 20 45 107 False\n",
      "Adding 20 48 110 False\n",
      "Adding 20 50 76 False\n",
      "Adding 20 52 114 False\n",
      "Adding 20 52 173 False\n",
      "Adding 20 54 138 False\n",
      "Adding 20 55 175 False\n",
      "Adding 20 55 220 False\n",
      "Adding 20 56 12 False\n",
      "Adding 20 59 135 False\n",
      "Adding 20 61 209 False\n",
      "Adding 20 68 127 False\n",
      "Adding 20 70 57 False\n",
      "Adding 20 71 99 False\n",
      "Adding 20 71 174 False\n",
      "Adding 20 73 201 False\n",
      "Adding 20 73 235 False\n",
      "Adding 20 74 64 False\n",
      "Adding 20 83 171 False\n",
      "Adding 20 84 161 False\n",
      "Adding 20 91 41 False\n",
      "Adding 20 96 117 False\n",
      "Adding 20 105 224 False\n",
      "Adding 20 112 159 False\n",
      "Adding 20 113 107 False\n",
      "Adding 20 115 17 False\n",
      "Adding 20 115 151 False\n",
      "Adding 20 115 226 False\n",
      "Adding 20 119 162 False\n",
      "Adding 20 121 103 False\n",
      "Adding 20 122 161 False\n",
      "Adding 20 127 29 False\n",
      "Adding 20 128 154 False\n",
      "Adding 20 129 241 False\n",
      "Adding 20 131 149 False\n",
      "Adding 20 135 93 False\n",
      "Adding 20 135 157 False\n",
      "Adding 20 136 62 False\n",
      "Adding 20 136 80 False\n",
      "Adding 20 140 143 False\n",
      "Adding 20 146 162 False\n",
      "Adding 20 148 212 False\n",
      "Adding 20 150 108 False\n",
      "Adding 20 152 48 False\n",
      "Adding 20 152 138 False\n",
      "Adding 20 156 30 False\n",
      "Adding 20 160 50 False\n",
      "Adding 20 161 12 False\n",
      "Adding 20 167 51 False\n",
      "Adding 20 167 229 False\n",
      "Adding 20 168 231 False\n",
      "Adding 20 169 236 False\n",
      "Adding 20 173 172 False\n",
      "Adding 20 176 49 False\n",
      "Adding 20 176 224 False\n",
      "Adding 20 181 173 False\n",
      "Adding 20 182 240 False\n",
      "Adding 20 189 26 False\n",
      "Adding 20 192 176 False\n",
      "Adding 20 194 116 False\n",
      "Adding 20 198 16 False\n",
      "Adding 20 198 120 False\n",
      "Adding 21 3 194 False\n",
      "Adding 21 7 194 False\n",
      "Adding 21 8 126 False\n",
      "Adding 21 9 139 False\n",
      "Adding 21 10 76 False\n",
      "Adding 21 10 80 False\n",
      "Adding 21 14 13 False\n",
      "Adding 21 15 78 False\n",
      "Adding 21 16 60 False\n",
      "Adding 21 22 0 False\n",
      "Adding 21 27 12 False\n",
      "Adding 21 27 117 False\n",
      "Adding 21 29 86 False\n",
      "Adding 21 30 89 False\n",
      "Adding 21 32 183 False\n",
      "Adding 21 33 130 False\n",
      "Adding 21 42 152 False\n",
      "Adding 21 44 191 False\n",
      "Adding 21 45 6 False\n",
      "Adding 21 45 153 False\n",
      "Adding 21 50 8 False\n",
      "Adding 21 50 38 False\n",
      "Adding 21 59 103 False\n",
      "Adding 21 60 183 False\n",
      "Adding 21 61 65 False\n",
      "Adding 21 62 89 False\n",
      "Adding 21 63 100 False\n",
      "Adding 21 66 192 False\n",
      "Adding 21 69 26 False\n",
      "Adding 21 69 81 False\n",
      "Adding 21 69 119 False\n",
      "Adding 21 69 234 False\n",
      "Adding 21 74 88 False\n",
      "Adding 21 77 71 False\n",
      "Adding 21 79 237 False\n",
      "Adding 21 80 58 False\n",
      "Adding 21 82 69 False\n",
      "Adding 21 86 0 False\n",
      "Adding 21 91 70 False\n",
      "Adding 21 91 191 False\n",
      "Adding 21 93 202 False\n",
      "Adding 21 94 48 False\n",
      "Adding 21 95 109 False\n",
      "Adding 21 97 36 False\n",
      "Adding 21 98 177 False\n",
      "Adding 21 99 175 False\n",
      "Adding 21 102 212 False\n",
      "Adding 21 103 47 False\n",
      "Adding 21 104 101 False\n",
      "Adding 21 105 202 False\n",
      "Adding 21 106 145 False\n",
      "Adding 21 107 192 False\n",
      "Adding 21 109 197 False\n",
      "Adding 21 111 184 False\n",
      "Adding 21 111 191 False\n",
      "Adding 21 111 235 False\n",
      "Adding 21 112 150 False\n",
      "Adding 21 112 218 False\n",
      "Adding 21 118 172 False\n",
      "Adding 21 118 226 False\n",
      "Adding 21 121 198 False\n",
      "Adding 21 127 216 False\n",
      "Adding 21 130 192 False\n",
      "Adding 21 131 223 False\n",
      "Adding 21 133 104 False\n",
      "Adding 21 134 11 False\n",
      "Adding 21 145 194 False\n",
      "Adding 21 146 173 False\n",
      "Adding 21 148 152 False\n",
      "Adding 21 150 37 False\n",
      "Adding 21 153 54 False\n",
      "Adding 21 153 82 False\n",
      "Adding 21 155 62 False\n",
      "Adding 21 156 169 False\n",
      "Adding 21 161 140 False\n",
      "Adding 21 161 151 False\n",
      "Adding 21 162 60 False\n",
      "Adding 21 162 160 False\n",
      "Adding 21 163 36 False\n",
      "Adding 21 164 98 False\n",
      "Adding 21 164 214 False\n",
      "Adding 21 165 82 False\n",
      "Adding 21 168 68 False\n",
      "Adding 21 170 137 False\n",
      "Adding 21 171 53 False\n",
      "Adding 21 176 113 False\n",
      "Adding 21 184 175 False\n",
      "Adding 21 185 95 False\n",
      "Adding 21 186 94 False\n",
      "Adding 21 186 238 False\n",
      "Adding 21 187 45 False\n",
      "Adding 21 189 183 False\n",
      "Adding 21 190 180 False\n",
      "Adding 21 193 203 False\n",
      "Adding 21 196 40 False\n",
      "Adding 21 196 121 False\n",
      "Adding 21 197 13 False\n",
      "Adding 21 198 120 False\n",
      "Adding 22 0 21 False\n",
      "Adding 22 0 24 False\n",
      "Adding 22 0 232 False\n",
      "Adding 22 3 72 False\n",
      "Adding 22 11 89 False\n",
      "Adding 22 13 10 False\n",
      "Adding 22 15 225 False\n",
      "Adding 22 17 104 False\n",
      "Adding 22 19 28 False\n",
      "Adding 22 19 39 False\n",
      "Adding 22 20 94 False\n",
      "Adding 22 23 107 False\n",
      "Adding 22 24 107 False\n",
      "Adding 22 24 225 False\n",
      "Adding 22 27 94 False\n",
      "Adding 22 27 159 False\n",
      "Adding 22 27 213 False\n",
      "Adding 22 32 82 False\n",
      "Adding 22 34 150 False\n",
      "Adding 22 36 81 False\n",
      "Adding 22 36 214 False\n",
      "Adding 22 37 59 False\n",
      "Adding 22 37 227 False\n",
      "Adding 22 38 90 False\n",
      "Adding 22 43 144 False\n",
      "Adding 22 46 210 False\n",
      "Adding 22 47 41 False\n",
      "Adding 22 48 127 False\n",
      "Adding 22 49 75 False\n",
      "Adding 22 49 168 False\n",
      "Adding 22 50 112 False\n",
      "Adding 22 55 117 False\n",
      "Adding 22 57 23 False\n",
      "Adding 22 58 22 False\n",
      "Adding 22 59 230 False\n",
      "Adding 22 60 234 False\n",
      "Adding 22 60 238 False\n",
      "Adding 22 63 199 False\n",
      "Adding 22 65 165 False\n",
      "Adding 22 65 235 False\n",
      "Adding 22 66 84 False\n",
      "Adding 22 69 176 False\n",
      "Adding 22 80 113 False\n",
      "Adding 22 81 28 False\n",
      "Adding 22 82 13 False\n",
      "Adding 22 89 188 False\n",
      "Adding 22 89 194 False\n",
      "Adding 22 91 202 False\n",
      "Adding 22 94 212 False\n",
      "Adding 22 95 26 False\n",
      "Adding 22 95 132 False\n",
      "Adding 22 97 74 False\n",
      "Adding 22 98 138 False\n",
      "Adding 22 99 78 False\n",
      "Adding 22 99 100 False\n",
      "Adding 22 100 68 False\n",
      "Adding 22 102 173 False\n",
      "Adding 22 104 170 False\n",
      "Adding 22 106 218 False\n",
      "Adding 22 107 42 False\n",
      "Adding 22 107 97 False\n",
      "Adding 22 107 149 False\n",
      "Adding 22 108 7 False\n",
      "Adding 22 109 58 False\n",
      "Adding 22 109 167 False\n",
      "Adding 22 112 185 False\n",
      "Adding 22 114 164 False\n",
      "Adding 22 118 75 False\n",
      "Adding 22 125 215 False\n",
      "Adding 22 126 146 False\n",
      "Adding 22 126 148 False\n",
      "Adding 22 126 246 False\n",
      "Adding 22 127 62 False\n",
      "Adding 22 132 156 False\n",
      "Adding 22 136 20 False\n",
      "Adding 22 136 41 False\n",
      "Adding 22 136 164 False\n",
      "Adding 22 139 127 False\n",
      "Adding 22 142 202 False\n",
      "Adding 22 143 15 False\n",
      "Adding 22 143 122 False\n",
      "Adding 22 143 200 False\n",
      "Adding 22 144 1 False\n",
      "Adding 22 144 3 False\n",
      "Adding 22 144 29 False\n",
      "Adding 22 148 70 False\n",
      "Adding 22 149 126 False\n",
      "Adding 22 154 231 False\n",
      "Adding 22 158 18 False\n",
      "Adding 22 158 59 False\n",
      "Adding 22 158 79 False\n",
      "Adding 22 158 142 False\n",
      "Adding 22 160 88 False\n",
      "Adding 22 160 107 False\n",
      "Adding 22 160 228 False\n",
      "Adding 22 163 16 False\n",
      "Adding 22 163 38 False\n",
      "Adding 22 169 85 False\n",
      "Adding 22 170 14 False\n",
      "Adding 22 175 119 False\n",
      "Adding 22 176 163 False\n",
      "Adding 22 177 88 False\n",
      "Adding 22 179 57 False\n",
      "Adding 22 181 83 False\n",
      "Adding 22 181 190 False\n",
      "Adding 22 182 65 False\n",
      "Adding 22 183 106 False\n",
      "Adding 22 189 117 False\n",
      "Adding 22 190 43 False\n",
      "Adding 22 191 143 False\n",
      "Adding 22 191 196 False\n",
      "Adding 22 192 118 False\n",
      "Adding 22 193 92 False\n",
      "Adding 22 193 199 False\n",
      "Adding 22 195 181 False\n",
      "Adding 22 196 213 False\n",
      "Adding 22 197 120 False\n",
      "Adding 22 197 139 False\n",
      "Adding 22 199 193 False\n",
      "Adding 23 3 230 False\n",
      "Adding 23 4 144 False\n",
      "Adding 23 5 57 False\n",
      "Adding 23 7 111 False\n",
      "Adding 23 7 134 False\n",
      "Adding 23 7 243 False\n",
      "Adding 23 13 195 False\n",
      "Adding 23 17 18 False\n",
      "Adding 23 20 52 False\n",
      "Adding 23 22 100 False\n",
      "Adding 23 23 171 False\n",
      "Adding 23 25 132 False\n",
      "Adding 23 26 74 False\n",
      "Adding 23 26 76 False\n",
      "Adding 23 28 147 False\n",
      "Adding 23 30 60 False\n",
      "Adding 23 31 104 False\n",
      "Adding 23 35 110 False\n",
      "Adding 23 35 210 False\n",
      "Adding 23 36 187 False\n",
      "Adding 23 36 220 False\n",
      "Adding 23 37 242 False\n",
      "Adding 23 38 204 False\n",
      "Adding 23 39 223 False\n",
      "Adding 23 48 190 False\n",
      "Adding 23 49 39 False\n",
      "Adding 23 51 35 False\n",
      "Adding 23 52 50 False\n",
      "Adding 23 57 109 False\n",
      "Adding 23 59 220 False\n",
      "Adding 23 63 185 False\n",
      "Adding 23 64 77 False\n",
      "Adding 23 68 240 False\n",
      "Adding 23 72 82 False\n",
      "Adding 23 72 205 False\n",
      "Adding 23 73 230 False\n",
      "Adding 23 74 10 False\n",
      "Adding 23 75 10 False\n",
      "Adding 23 79 35 False\n",
      "Adding 23 80 0 False\n",
      "Adding 23 80 77 False\n",
      "Adding 23 81 50 False\n",
      "Adding 23 82 29 False\n",
      "Adding 23 82 71 False\n",
      "Adding 23 82 238 False\n",
      "Adding 23 83 163 False\n",
      "Adding 23 92 190 False\n",
      "Adding 23 94 227 False\n",
      "Adding 23 98 193 False\n",
      "Adding 23 100 30 False\n",
      "Adding 23 101 5 False\n",
      "Adding 23 106 162 False\n",
      "Adding 23 107 39 False\n",
      "Adding 23 108 237 False\n",
      "Adding 23 109 105 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 23 115 115 False\n",
      "Adding 23 115 127 False\n",
      "Adding 23 116 56 False\n",
      "Adding 23 118 61 False\n",
      "Adding 23 127 162 False\n",
      "Adding 23 128 218 False\n",
      "Adding 23 129 15 False\n",
      "Adding 23 129 139 False\n",
      "Adding 23 129 200 False\n",
      "Adding 23 131 127 False\n",
      "Adding 23 133 41 False\n",
      "Adding 23 136 33 False\n",
      "Adding 23 136 60 False\n",
      "Adding 23 143 43 False\n",
      "Adding 23 147 18 False\n",
      "Adding 23 148 96 False\n",
      "Adding 23 149 93 False\n",
      "Adding 23 150 46 False\n",
      "Adding 23 150 157 False\n",
      "Adding 23 151 248 False\n",
      "Adding 23 153 53 False\n",
      "Adding 23 154 195 False\n",
      "Adding 23 160 249 False\n",
      "Adding 23 162 156 False\n",
      "Adding 23 163 41 False\n",
      "Adding 23 170 127 False\n",
      "Adding 23 171 0 False\n",
      "Adding 23 178 87 False\n",
      "Adding 23 180 61 False\n",
      "Adding 23 180 159 False\n",
      "Adding 23 180 235 False\n",
      "Adding 23 181 193 False\n",
      "Adding 23 182 120 False\n",
      "Adding 23 182 181 False\n",
      "Adding 23 185 74 False\n",
      "Adding 23 188 23 False\n",
      "Adding 23 190 78 False\n",
      "Adding 23 190 126 False\n",
      "Adding 23 193 105 False\n",
      "Adding 23 193 201 False\n",
      "Adding 23 194 56 False\n",
      "Adding 23 196 187 False\n",
      "Adding 23 198 247 False\n",
      "Adding 24 1 24 False\n",
      "Adding 24 2 34 False\n",
      "Adding 24 4 13 False\n",
      "Adding 24 4 147 False\n",
      "Adding 24 5 120 False\n",
      "Adding 24 6 70 False\n",
      "Adding 24 7 161 False\n",
      "Adding 24 10 68 False\n",
      "Adding 24 12 242 False\n",
      "Adding 24 13 219 False\n",
      "Adding 24 14 132 False\n",
      "Adding 24 15 166 False\n",
      "Adding 24 15 228 False\n",
      "Adding 24 16 31 False\n",
      "Adding 24 16 143 False\n",
      "Adding 24 17 38 False\n",
      "Adding 24 20 217 False\n",
      "Adding 24 21 114 False\n",
      "Adding 24 23 150 False\n",
      "Adding 24 24 218 False\n",
      "Adding 24 25 65 False\n",
      "Adding 24 25 174 False\n",
      "Adding 24 28 173 False\n",
      "Adding 24 29 19 False\n",
      "Adding 24 29 72 False\n",
      "Adding 24 30 157 False\n",
      "Adding 24 32 2 False\n",
      "Adding 24 33 237 False\n",
      "Adding 24 34 88 False\n",
      "Adding 24 45 16 False\n",
      "Adding 24 54 195 False\n",
      "Adding 24 55 240 False\n",
      "Adding 24 56 148 False\n",
      "Adding 24 59 137 False\n",
      "Adding 24 61 107 False\n",
      "Adding 24 61 193 False\n",
      "Adding 24 64 34 False\n",
      "Adding 24 66 39 False\n",
      "Adding 24 66 71 False\n",
      "Adding 24 67 40 False\n",
      "Adding 24 68 143 False\n",
      "Adding 24 69 231 False\n",
      "Adding 24 71 136 False\n",
      "Adding 24 71 223 False\n",
      "Adding 24 72 87 False\n",
      "Adding 24 73 134 False\n",
      "Adding 24 73 166 False\n",
      "Adding 24 74 151 False\n",
      "Adding 24 75 125 False\n",
      "Adding 24 78 101 False\n",
      "Adding 24 79 231 False\n",
      "Adding 24 81 56 False\n",
      "Adding 24 81 227 False\n",
      "Adding 24 84 158 False\n",
      "Adding 24 85 220 False\n",
      "Adding 24 90 123 False\n",
      "Adding 24 92 16 False\n",
      "Adding 24 93 11 False\n",
      "Adding 24 93 218 False\n",
      "Adding 24 95 8 False\n",
      "Adding 24 97 235 False\n",
      "Adding 24 98 88 False\n",
      "Adding 24 102 161 False\n",
      "Adding 24 103 119 False\n",
      "Adding 24 104 88 False\n",
      "Adding 24 107 143 False\n",
      "Adding 24 107 241 False\n",
      "Adding 24 110 131 False\n",
      "Adding 24 111 3 False\n",
      "Adding 24 112 214 False\n",
      "Adding 24 114 132 False\n",
      "Adding 24 117 109 False\n",
      "Adding 24 120 11 False\n",
      "Adding 24 124 2 False\n",
      "Adding 24 124 89 False\n",
      "Adding 24 126 164 False\n",
      "Adding 24 128 132 False\n",
      "Adding 24 129 155 False\n",
      "Adding 24 131 45 False\n",
      "Adding 24 134 155 False\n",
      "Adding 24 137 40 False\n",
      "Adding 24 137 167 False\n",
      "Adding 24 137 238 False\n",
      "Adding 24 139 216 False\n",
      "Adding 24 141 20 False\n",
      "Adding 24 143 102 False\n",
      "Adding 24 150 64 False\n",
      "Adding 24 152 31 False\n",
      "Adding 24 155 142 False\n",
      "Adding 24 156 228 False\n",
      "Adding 24 157 63 False\n",
      "Adding 24 157 168 False\n",
      "Adding 24 158 164 False\n",
      "Adding 24 159 67 False\n",
      "Adding 24 160 226 False\n",
      "Adding 24 161 97 False\n",
      "Adding 24 161 220 False\n",
      "Adding 24 163 186 False\n",
      "Adding 24 165 73 False\n",
      "Adding 24 169 48 False\n",
      "Adding 24 169 173 False\n",
      "Adding 24 170 139 False\n",
      "Adding 24 171 118 False\n",
      "Adding 24 175 115 False\n",
      "Adding 24 177 192 False\n",
      "Adding 24 178 41 False\n",
      "Adding 24 179 169 False\n",
      "Adding 24 180 40 False\n",
      "Adding 24 182 2 False\n",
      "Adding 24 185 52 False\n",
      "Adding 24 185 72 False\n",
      "Adding 24 187 99 False\n",
      "Adding 24 189 173 False\n",
      "Adding 24 191 61 False\n",
      "Adding 24 192 25 False\n",
      "Adding 24 195 14 False\n",
      "Adding 24 197 224 False\n",
      "Adding 24 198 109 False\n",
      "Adding 24 198 195 False\n",
      "Adding 24 199 164 False\n",
      "Adding 25 0 244 False\n",
      "Adding 25 1 77 False\n",
      "Adding 25 9 120 False\n",
      "Adding 25 10 57 False\n",
      "Adding 25 11 160 False\n",
      "Adding 25 12 6 False\n",
      "Adding 25 12 71 False\n",
      "Adding 25 14 105 False\n",
      "Adding 25 15 153 False\n",
      "Adding 25 16 239 False\n",
      "Adding 25 20 23 False\n",
      "Adding 25 20 78 False\n",
      "Adding 25 21 245 False\n",
      "Adding 25 23 236 False\n",
      "Adding 25 26 76 False\n",
      "Adding 25 29 102 False\n",
      "Adding 25 37 63 False\n",
      "Adding 25 39 112 False\n",
      "Adding 25 39 175 False\n",
      "Adding 25 40 25 False\n",
      "Adding 25 42 78 False\n",
      "Adding 25 44 158 False\n",
      "Adding 25 44 187 False\n",
      "Adding 25 47 47 False\n",
      "Adding 25 51 148 False\n",
      "Adding 25 54 41 False\n",
      "Adding 25 54 155 False\n",
      "Adding 25 54 226 False\n",
      "Adding 25 57 88 False\n",
      "Adding 25 57 107 False\n",
      "Adding 25 60 14 False\n",
      "Adding 25 60 125 False\n",
      "Adding 25 61 88 False\n",
      "Adding 25 66 92 False\n",
      "Adding 25 69 110 False\n",
      "Adding 25 70 19 False\n",
      "Adding 25 70 30 False\n",
      "Adding 25 71 49 False\n",
      "Adding 25 71 147 False\n",
      "Adding 25 71 242 False\n",
      "Adding 25 72 98 False\n",
      "Adding 25 79 46 False\n",
      "Adding 25 80 188 False\n",
      "Adding 25 81 90 False\n",
      "Adding 25 83 194 False\n",
      "Adding 25 84 128 False\n",
      "Adding 25 85 177 False\n",
      "Adding 25 85 219 False\n",
      "Adding 25 86 40 False\n",
      "Adding 25 89 86 False\n",
      "Adding 25 89 91 False\n",
      "Adding 25 90 181 False\n",
      "Adding 25 90 202 False\n",
      "Adding 25 92 23 False\n",
      "Adding 25 93 151 False\n",
      "Adding 25 94 244 False\n",
      "Adding 25 95 80 False\n",
      "Adding 25 97 118 False\n",
      "Adding 25 98 150 False\n",
      "Adding 25 99 165 False\n",
      "Adding 25 100 192 False\n",
      "Adding 25 101 199 False\n",
      "Adding 25 103 189 False\n",
      "Adding 25 111 40 False\n",
      "Adding 25 111 157 False\n",
      "Adding 25 111 176 False\n",
      "Adding 25 113 105 False\n",
      "Adding 25 113 132 False\n",
      "Adding 25 122 193 False\n",
      "Adding 25 129 188 False\n",
      "Adding 25 130 107 False\n",
      "Adding 25 131 36 False\n",
      "Adding 25 136 136 False\n",
      "Adding 25 137 105 False\n",
      "Adding 25 138 88 False\n",
      "Adding 25 144 155 False\n",
      "Adding 25 149 88 False\n",
      "Adding 25 149 209 False\n",
      "Adding 25 155 176 False\n",
      "Adding 25 156 185 False\n",
      "Adding 25 158 151 False\n",
      "Adding 25 159 203 False\n",
      "Adding 25 160 217 False\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_X_and_Y_neg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X.shape)\n",
    "print (Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Data/boosting_RTL/X_neg\", X)\n",
    "np.savetxt(\"Data/boosting_RTL/Y_neg\", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(951868, 9)\n",
      "(951868,)\n"
     ]
    }
   ],
   "source": [
    "# X_train = np.loadtxt(\"Data/boosting_RTL/X_pos\")\n",
    "# Y_train = np.loadtxt(\"Data/boosting_RTL/Y_pos\")\n",
    "# print (X_train.shape)\n",
    "# print (Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train = create_X_and_Y_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"Data/train_boosting/X\", X_train)\n",
    "# np.savetxt(\"Data/train_boosting/Y\", Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test , Y_test  = create_X_and_Y_test ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"Data/test_boosting/X\" , X_test)\n",
    "# np.savetxt(\"Data/test_boosting/Y\" , Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_X_and_Y_train():\n",
    "#     X_train = np.zeros([0, OBSERVED_DAYS], dtype=float)\n",
    "#     Y_train = np.zeros([0])\n",
    "#     for t in tqdm (range (OBSERVED_DAYS, celled_data.shape[0] -\n",
    "#                                          TESTING_DAYS -\n",
    "#                                          DAYS_TO_PREDICT_BEFORE)):\n",
    "#         for i in range (celled_data.shape[2]):\n",
    "#             for j in range (celled_data.shape[3]):\n",
    "#                 X = celled_data[(t - OBSERVED_DAYS):t, 0, i, j]\n",
    "#                 Y = np.sum((celled_data[(t + DAYS_TO_PREDICT_AFTER):\n",
    "#                                         (t + DAYS_TO_PREDICT_BEFORE), 0, i, j] >\n",
    "#                             HEAVY_QUAKE_THRES)) > 0\n",
    "#                 if (np.sum(X) > 0):\n",
    "#                     X_train = np.concatenate((X_train, np.expand_dims(X, axis=0)), axis=0)\n",
    "#                     Y_train = np.concatenate((Y_train, np.expand_dims(Y, axis=0)), axis=0)\n",
    "\n",
    "#     #             print (X_train.shape)\n",
    "#     #             print (Y_train.shape)\n",
    "#     return X_train, Y_train\n",
    "\n",
    "# def create_X_and_Y_test():\n",
    "#     X_test = np.zeros([0, OBSERVED_DAYS], dtype=float)\n",
    "#     Y_test = np.zeros([0])\n",
    "#     for t in tqdm (range (celled_data.shape[0] -\n",
    "#                           TESTING_DAYS +\n",
    "#                           OBSERVED_DAYS,\n",
    "#                           celled_data.shape[0] -\n",
    "#                           DAYS_TO_PREDICT_BEFORE)):\n",
    "#         for i in range (celled_data.shape[2]):\n",
    "#             for j in range (celled_data.shape[3]):\n",
    "#                 X = celled_data[(t - OBSERVED_DAYS):t, 0, i, j]\n",
    "#                 Y = np.sum((celled_data[(t + DAYS_TO_PREDICT_AFTER):\n",
    "#                                         (t + DAYS_TO_PREDICT_BEFORE), 0, i, j] >\n",
    "#                             HEAVY_QUAKE_THRES)) > 0\n",
    "#                 if (np.sum(X) > 0):\n",
    "#                     X_test = np.concatenate((X_test, np.expand_dims(X, axis=0)), axis=0)\n",
    "#                     Y_test = np.concatenate((Y_test, np.expand_dims(Y, axis=0)), axis=0)\n",
    "\n",
    "#     #             print (X_test.shape)\n",
    "#     #             print (Y_test.shape)\n",
    "#     return X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.loadtxt(\"Data/train_boosting/X\")\n",
    "Y_train = np.loadtxt(\"Data/train_boosting/Y\")\n",
    "print (X_train.shape)\n",
    "print (Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test  = np.loadtxt(\"Data/test_boosting/X\")\n",
    "Y_test  = np.loadtxt(\"Data/test_boosting/Y\")\n",
    "print (X_test .shape)\n",
    "print (Y_test .shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.zeros([(celled_data.shape[0] -\n",
    "#                      OBSERVED_DAYS -\n",
    "#                      TESTING_DAYS -\n",
    "#                      DAYS_TO_PREDICT_BEFORE) *\n",
    "#                     N_CELLS_HOR *\n",
    "#                     N_CELLS_VER,\n",
    "#                     OBSERVED_DAYS], \n",
    "#                    dtype=float)\n",
    "# Y_train = np.zeros([(celled_data.shape[0] -\n",
    "#                      OBSERVED_DAYS -\n",
    "#                      TESTING_DAYS -\n",
    "#                      DAYS_TO_PREDICT_BEFORE) *\n",
    "#                     N_CELLS_HOR *\n",
    "#                     N_CELLS_VER,\n",
    "#                     OBSERVED_DAYS])\n",
    "\n",
    "# for t in tqdm (range (OBSERVED_DAYS, celled_data.shape[0] -\n",
    "#                                      TESTING_DAYS -\n",
    "#                                      DAYS_TO_PREDICT_BEFORE)):\n",
    "#     for i in range (celled_data.shape[2]):\n",
    "#         for j in range (celled_data.shape[3]):\n",
    "#             X_train[t * N_CELLS_HOR * N_CELLS_VER + i * N_CELLS_VER + j] = \\\n",
    "#             celled_data[(t - OBSERVED_DAYS):t, 0, i, j]\n",
    "            \n",
    "#             Y_train[t * N_CELLS_HOR * N_CELLS_VER + i * N_CELLS_VER + j] = \\\n",
    "#             np.sum(celled_data[(t + DAYS_TO_PREDICT_AFTER):\n",
    "#                                (t + DAYS_TO_PREDICT_BEFORE), 0, i, j] >\n",
    "#                    HEAVY_QUAKE_THRES) > 0\n",
    "# #             X_train = np.concatenate((X_train, np.expand_dims(X, axis=0)), axis=0)\n",
    "# #             Y_train = np.concatenate((Y_train, np.expand_dims(Y, axis=0)), axis=0)\n",
    "            \n",
    "# #             print (X_train.shape)\n",
    "# #             print (Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим саму сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtrain = xgb.DMatrix (X_train, label=Y_train)\n",
    "# dtest  = xgb.DMatrix (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_clf = xgb.XGBClassifier(objective='binary:logistic', n_jobs=4)\n",
    "boost_clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = boost_clf.predict(X_test)\n",
    "\n",
    "ROC_AUC_score = roc_auc_score(Y_test, prediction)\n",
    "Avg_precision = average_precision_score(Y_test, prediction)\n",
    "\n",
    "print (\"ROC_AUC = \", ROC_AUC_score, \"Avg_precision = \", Avg_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_clf = xgb.XGBClassifier(objective='binary:logistic', n_jobs=16)\n",
    "\n",
    "param_grid = {'n_jobs'      :[16],\n",
    "              'objective'   :['binary:logistic'],\n",
    "              'max_depth'   :[1, 3, 5, 11],\n",
    "              'n_estimators':[2, 10, 20],\n",
    "              'booster'     :['gbtree', 'gblinear']\n",
    "             } # might be useful to try to change some other params\n",
    "grid_cv = GridSearchCV (boost_clf, param_grid, scoring=make_scorer(average_precision_score), cv=5)\n",
    "\n",
    "grid_cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best params:')\n",
    "print (grid_cv.best_params_)\n",
    "\n",
    "best_boost_clf = xgb.XGBClassifier(n_jobs      =grid_cv.best_params_['n_jobs'],\n",
    "                                   objective   =grid_cv.best_params_['objective'],\n",
    "                                   max_depth   =grid_cv.best_params_['max_depth'],\n",
    "                                   n_estimators=grid_cv.best_params_['n_estimators'],\n",
    "                                   booster     =grid_cv.best_params_['booster'])\n",
    "\n",
    "best_boost_clf.fit (X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = best_boost_clf.predict(X_test)\n",
    "\n",
    "ROC_AUC_score = roc_auc_score(Y_test, prediction)\n",
    "Avg_precision = average_precision_score(Y_test, prediction)\n",
    "\n",
    "print (\"ROC_AUC = \", ROC_AUC_score, \"Avg_precision = \", Avg_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (4, 21, 2):\n",
    "    boost_clf = xgb.XGBClassifier(n_jobs      = 4,\n",
    "                                  objective   = 'binary:logistic',\n",
    "                                  max_depth   = 10,\n",
    "                                  n_estimators= 20,\n",
    "                                  booster     = 'gbtree')\n",
    "    boost_clf.fit (X_train[:, 0:i], Y_train)\n",
    "    \n",
    "    prediction = boost_clf.predict(X_test[:, 0:i])\n",
    "    ROC_AUC_score = roc_auc_score          (Y_test, prediction)\n",
    "    Avg_precision = average_precision_score(Y_test, prediction)\n",
    "    print (\"i = \", i, \"ROC_AUC = \", ROC_AUC_score, \"Avg_precision = \", Avg_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_ROCinfo (model, dataLoader, device, alpha=0.5, n_dots=101):\n",
    "#     model = model.to(device)\n",
    "    \n",
    "    \n",
    "#     threshold_massive = np.linspace (0, n_dots-1, n_dots, dtype=int)\n",
    "#     TP_massive = np.zeros (n_dots)\n",
    "#     FP_massive = np.zeros (n_dots)\n",
    "#     FN_massive = np.zeros (n_dots)\n",
    "#     TN_massive = np.zeros (n_dots)\n",
    "    \n",
    "#     for data in dataLoader:\n",
    "#         inputs = data[0].to(device)\n",
    "#         labels = data[1].to(device)\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         for threshold in threshold_massive:\n",
    "#             prediction = (outputs[:, 1, :, :].unsqueeze(1))>(threshold/n_dots)\n",
    "#             TP_massive[threshold] += torch.sum (prediction       * labels      ).float()\n",
    "#             FP_massive[threshold] += torch.sum (prediction       * (1 - labels)).float()\n",
    "#             FN_massive[threshold] += torch.sum ((1 - prediction) * labels      ).float()\n",
    "#             TN_massive[threshold] += torch.sum ((1 - prediction) * (1 - labels)).float()\n",
    "            \n",
    "#     threshold_massive = threshold_massive / (n_dots-1)\n",
    "#     precision_massive = TP_massive / (TP_massive + FP_massive)\n",
    "#     TPR_massive       = TP_massive / (TP_massive + FN_massive)\n",
    "#     FPR_massive       = FP_massive / (FP_massive + TN_massive)\n",
    "\n",
    "#     sum_events = TP_massive[int(len(TP_massive)/2)] + FP_massive[int(len(FP_massive)/2)] + FN_massive[int(len(FN_massive)/2)] + TN_massive[int(len(TN_massive)/2)] \n",
    "#     print ('TP = ', round(TP_massive[int(len(TP_massive)/2)] / sum_events, 6), '%')\n",
    "#     print ('FP = ', round(FP_massive[int(len(FP_massive)/2)] / sum_events, 6), '%')\n",
    "#     print ('FN = ', round(FN_massive[int(len(FN_massive)/2)] / sum_events, 6), '%')\n",
    "#     print ('TN = ', round(TN_massive[int(len(TN_massive)/2)] / sum_events, 6), '%')\n",
    "    \n",
    "#     if (SAVE_INFO == True):\n",
    "#         print ('TP = ', round(TP_massive[int(len(TP_massive)/2)] / sum_events, 6), '%', file=INFO_FILE)\n",
    "#         print ('FP = ', round(FP_massive[int(len(FP_massive)/2)] / sum_events, 6), '%', file=INFO_FILE)\n",
    "#         print ('FN = ', round(FN_massive[int(len(FN_massive)/2)] / sum_events, 6), '%', file=INFO_FILE)\n",
    "#         print ('TN = ', round(TN_massive[int(len(TN_massive)/2)] / sum_events, 6), '%', file=INFO_FILE)\n",
    "    \n",
    "#     # plot 1 precision\n",
    "#     fig1 = plt.figure(figsize=(10, 6))\n",
    "\n",
    "#     axes = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "#     axes.plot(threshold_massive, precision_massive, color='green', marker='^')\n",
    "\n",
    "#     axes.set_xlabel('threshold')\n",
    "#     axes.set_ylabel('precision')\n",
    "\n",
    "#     if (SAVE_INFO == True):\n",
    "#         plt.savefig(EXPERIMENT_DIR + 'Precision_from_threshold.png', format='png', dpi=100)\n",
    "#     plt.show()\n",
    "    \n",
    "#     # plot 2 recall\n",
    "#     fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "#     axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "#     axes.plot(threshold_massive, TPR_massive, color='green', marker='^')\n",
    "\n",
    "#     axes.set_xlabel('threshold')\n",
    "#     axes.set_ylabel('recall')\n",
    "    \n",
    "#     if (SAVE_INFO == True):\n",
    "#         plt.savefig(EXPERIMENT_DIR + 'Recall_from_threshold.png', format='png', dpi=100)\n",
    "#     plt.show()\n",
    "    \n",
    "#     # plot 3 ROC-curve\n",
    "#     fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "#     axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "#     axes.plot(FPR_massive, TPR_massive, 'orange', marker = '^')\n",
    "#     axes.plot (range(2), range(2), 'grey', ls='--')\n",
    "\n",
    "#     axes.set_xlabel('FPR')\n",
    "#     axes.set_ylabel('TPR (recall)')\n",
    "#     axes.set_title('ROC-curve')\n",
    "\n",
    "#     if (SAVE_INFO == True):\n",
    "#         plt.savefig(EXPERIMENT_DIR + 'ROC_curve.png', format='png', dpi=100)\n",
    "#     plt.show()\n",
    "    \n",
    "#     del model\n",
    "#     del inputs\n",
    "#     del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_INFO_to_file():\n",
    "    if (SAVE_INFO == True):\n",
    "        print ('ORIGIN_LATITUDE        =', ORIGIN_LATITUDE                       , file=INFO_FILE)\n",
    "        print ('ORIGIN_LONGITUDE       =', ORIGIN_LONGITUDE                      , file=INFO_FILE)\n",
    "        print ('LEFT_BORDER            =', LEFT_BORDER                           , file=INFO_FILE)\n",
    "        print ('RIGHT_BORDER           =', RIGHT_BORDER                          , file=INFO_FILE)\n",
    "        print ('DOWN_BORDER            =', DOWN_BORDER                           , file=INFO_FILE)\n",
    "        print ('UP_BORDER              =', UP_BORDER                             , file=INFO_FILE)\n",
    "        print ('N_CELLS_HOR            =', N_CELLS_HOR                           , file=INFO_FILE)\n",
    "        print ('N_CELLS_VER            =', N_CELLS_VER                           , file=INFO_FILE)\n",
    "        print (' '                                                               , file=INFO_FILE)\n",
    "        print ('OBSERVED_DAYS          =', OBSERVED_DAYS                         , file=INFO_FILE)\n",
    "        print ('DAYS_TO_PREDICT        =', DAYS_TO_PREDICT                       , file=INFO_FILE)\n",
    "        print ('STEP                   =', STEP                                  , file=INFO_FILE)\n",
    "        print ('TESTING_DAYS           =', TESTING_DAYS                          , file=INFO_FILE)\n",
    "        print ('HEAVY_QUAKE_THRES      =', HEAVY_QUAKE_THRES                     , file=INFO_FILE)\n",
    "        print ('LEARNING_RATE          =', LEARNING_RATE                         , file=INFO_FILE)\n",
    "        print ('N_EPOCHS               =', N_EPOCHS                              , file=INFO_FILE)\n",
    "        print ('EARTHQUAKE_WEIGHT      =', EARTHQUAKE_WEIGHT                     , file=INFO_FILE)\n",
    "        print ('TRAIN_SHAPE            =', earthquakes_dataset_train.data.shape  , file=INFO_FILE)\n",
    "        print ('TEST__SHAPE            =', earthquakes_dataset_test .result.shape, file=INFO_FILE)\n",
    "        \n",
    "#         print ('', , file=INFO_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SAVE_INFO == True):\n",
    "    INFO_FILE = open (EXPERIMENT_DIR + 'INFO.txt', 'w')\n",
    "    \n",
    "if (SAVE_INFO == True):\n",
    "    print_INFO_to_file()\n",
    "    \n",
    "print_ROCinfo (earthquake_network,\n",
    "               earthquakes_dataloader_test,\n",
    "               DEVICE,\n",
    "               ) \n",
    "\n",
    "if (SAVE_INFO == True):\n",
    "    INFO_FILE.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvNetwork_MSE (nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(ConvNetwork_CE, self).__init__()\n",
    "        \n",
    "#         self.features = nn.Sequential()\n",
    "        \n",
    "#         self.features.add_module('conv1', conv_block(     OBSERVED_DAYS    , int (OBSERVED_DAYS/2 ), 3))\n",
    "#         self.features.add_module('conv2', conv_block(int (OBSERVED_DAYS/2 ), int (OBSERVED_DAYS/4 ), 3))\n",
    "#         self.features.add_module('conv3', conv_block(int (OBSERVED_DAYS/4 ), int (OBSERVED_DAYS/8 ), 3))\n",
    "#         self.features.add_module('conv4', conv_block(int (OBSERVED_DAYS/8 ), int (OBSERVED_DAYS/16), 3))\n",
    "#         self.features.add_module('conv5', conv_block(int (OBSERVED_DAYS/16),                      1, 3))\n",
    "        \n",
    "#         # might be a good idea to add an extra full connected layer\n",
    "        \n",
    "#     def forward(self, x):\n",
    "# #         print ('input  : ', x.shape)\n",
    "#         x = self.features(x)\n",
    "# #         print ('output : ', x.shape)\n",
    "#         return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_network_MSE(network, \n",
    "#                       device,\n",
    "#                       dataloader_train,\n",
    "#                       dataloader_test,\n",
    "#                       epochs=164,\n",
    "#                       learning_rate=0.1):\n",
    "    \n",
    "#     train_acc = []\n",
    "#     test_acc  = []\n",
    "#     net = network.to(device)\n",
    "\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, weight_decay=0.0001, momentum=0.9)\n",
    "\n",
    "    \n",
    "#     for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
    "#         if epoch == epochs/2:\n",
    "#             optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate/10, weight_decay=0.0001, momentum=0.9) \n",
    "#         elif epoch == epochs*3/4:\n",
    "#             optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate/100, weight_decay=0.0001, momentum=0.9) \n",
    "        \n",
    "#         net = net.train()        \n",
    "#         epoch_accuracy = 0.0\n",
    "#         epoch_elems = 0\n",
    "#         for data in dataloader_train:\n",
    "            \n",
    "#             inputs = data[0].to(device)\n",
    "#             labels = data[1].to(device)\n",
    "# #             print ('inputs_shape = ', inputs.shape)\n",
    "# #             print ('labels_shape = ', labels.shape)\n",
    "\n",
    "#             # zero the parameter gradients\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # forward + backward + optimize\n",
    "#             outputs = net(inputs)\n",
    "# #             print ('outputs : ', outputs.dtype)\n",
    "# #             print ('labels  : ', labels.dtype)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "# #             print (loss)\n",
    "#             epoch_elems    += labels.shape[0]\n",
    "#             epoch_accuracy += loss.item()*labels.shape[0]\n",
    "\n",
    "#         epoch_accuracy /= epoch_elems\n",
    "#         train_acc.append(epoch_accuracy)\n",
    "        \n",
    "        \n",
    "#         # calculating test accuracy\n",
    "#         epoch_accuracy = 0.0\n",
    "#         epoch_elems = 0\n",
    "#         for data in dataloader_test:\n",
    "#             inputs = data[0].to(device)\n",
    "#             labels = data[1].to(device)   \n",
    "#             outputs = net(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             epoch_elems    += labels.shape[0]\n",
    "#             epoch_accuracy += loss.item()*labels.shape[0]\n",
    "\n",
    "#         epoch_accuracy /= epoch_elems\n",
    "#         test_acc.append(epoch_accuracy)\n",
    "        \n",
    "        \n",
    "#         print('Epoch : ', epoch, 'acc_train : ', round (train_acc[-1], 4), 'acc_test : ', round (test_acc[-1], 4))\n",
    "\n",
    "#     print('Finished Training')\n",
    "    \n",
    "#     plt.plot(train_acc, label='Train')\n",
    "#     plt.plot(test_acc , label='Test' )\n",
    "#     plt.legend()\n",
    "#     plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earthquake_netowrk = ConvNetwork_MSE ()\n",
    "# train_network_MSE (earthquake_netowrk,\n",
    "#                    torch.device(DEVICE),\n",
    "#                    earthquakes_dataloader_train,\n",
    "#                    earthquakes_dataloader_test,\n",
    "#                    epochs=200,\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_mistake (x):\n",
    "#     assert (torch.sum((x < 0.0) + (x > 1.0)) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# print ('input  : ', input.shape, input.dtype)\n",
    "# print (input)\n",
    "# print (torch.sum (input, dim = 0))\n",
    "# print ('target : ', target.shape, target.dtype)\n",
    "# print (target)\n",
    "# output = loss(input, target)\n",
    "# output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# class someDataset (Dataset):\n",
    "#     def __init__(self):\n",
    "#         self.data = torch.ones ([100, 32, 10, 10])\n",
    "#         self.labels = torch.ones ([100, 1, 10, 10])\n",
    "#         self.len  = self.data.shape[0]\n",
    "        \n",
    "#         print (self.data.shape)\n",
    "#         print (self.labels.shape)\n",
    "        \n",
    "#     def __len__ (self):\n",
    "#         return self.len\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         print ('data_shape = ', self.data[idx].shape)\n",
    "#         print ('result_shape = ', self.labels[idx].shape)\n",
    "#         return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some_dataset = someDataset()\n",
    "# dataloader = DataLoader (some_dataset,\n",
    "#                          batch_size=32,\n",
    "#                          shuffle=True,\n",
    "#                          num_workers=1,\n",
    "#                          )\n",
    "\n",
    "# for i, batch in enumerate(dataloader, 0):\n",
    "#     data = batch[0]\n",
    "#     print (i, 'data ', data.shape)\n",
    "#     labels = batch[1]\n",
    "#     print (i, 'labels ', labels.shape)\n",
    "    \n",
    "\n",
    "\n",
    "# # eartquakes_dataloader_train = DataLoader(earthquakes_dataset_train,\n",
    "# #                                          batch_size=33,\n",
    "# #                                          shuffle=True,\n",
    "# #                                          num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
