{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "import datetime as dt\n",
    "\n",
    "# Results presentation\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NN related stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_INFO = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SAVE_INFO == True):\n",
    "    DATA_DIR = 'Experiments/'\n",
    "    EXPERIMENT_DIR = DATA_DIR + 'LSTM_dependency'\n",
    "    os.makedirs(EXPERIMENT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение датасета по дням и по клеткам в сетке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT_BORDER = 0\n",
    "RIGHT_BORDER = 2000\n",
    "DOWN_BORDER = 0\n",
    "UP_BORDER = 2500\n",
    "\n",
    "N_CELLS_HOR = 200\n",
    "N_CELLS_VER = 250\n",
    "\n",
    "R_CIRCLE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "celled_data = torch.load(\"Data/celled_data_\" +\n",
    "                         str(N_CELLS_HOR) + \"x\" +\n",
    "                         str(N_CELLS_VER))\n",
    "\n",
    "celled_data_cir = torch.load (\"Data/celled_data_cir_R=\" +\n",
    "                              str(R_CIRCLE) + \"_\" +\n",
    "                              str(N_CELLS_HOR) + \"x\" +\n",
    "                              str(N_CELLS_VER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_one_day_image (tensor, day):\n",
    "    plt.imshow (tensor[day].squeeze(0), cmap=plt.cm.afmhot_r)\n",
    "    plt.colorbar()\n",
    "    \n",
    "def show_one_day_quakes (tensor, day):\n",
    "    state = tensor[day].squeeze(0)\n",
    "    print (state.shape)\n",
    "    X = []\n",
    "    Y = []\n",
    "    M = []\n",
    "    for i in range(state.shape[0]):\n",
    "        for j in range(state.shape[1]):\n",
    "            if (state[i][j] != 0):\n",
    "                X.append(i)\n",
    "                Y.append(j)\n",
    "                M.append(state[i][j].item())\n",
    "    print (X)\n",
    "    print (Y)\n",
    "    print (M)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 10))\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    plt.axis([0, state.shape[0], 0, state.shape[1]])\n",
    "    axes.scatter(X, Y, s=500, c=M, marker='.', cmap=plt.cm.Reds)\n",
    "#     plt.colorbar()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 250])\n",
      "[74, 88, 138, 149]\n",
      "[89, 87, 169, 177]\n",
      "[2.5, 2.5999999046325684, 3.700000047683716, 2.9000000953674316]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_one_day_quakes (celled_data, 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "DEVICE_ID = 3\n",
    "DEVICE = torch.device('cuda:%d' % DEVICE_ID)\n",
    "print (DEVICE)\n",
    "# torch.cuda.set_device(DEVICE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_accuracy (input, target, threshold):\n",
    "    true = ((input>threshold) == target)\n",
    "    sum = torch.sum (true.float())\n",
    "    return sum/input.shape[0]/input.shape[1]/input.shape[2]/input.shape[3]\n",
    "\n",
    "def my_precision (input, target, threshold):\n",
    "    TP = torch.sum (((input>threshold) * target      ).float())\n",
    "    FP = torch.sum (((input>threshold) * (1 - target)).float())\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "def my_recall (input, target, threshold):\n",
    "    TP = torch.sum ((     (input>threshold)  * target).float())\n",
    "    FN = torch.sum (((1 - (input>threshold)) * target).float())\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def my_precision_recall (input, target, threshold):\n",
    "    TP = torch.sum ((     (input>threshold)  * target      ).float())\n",
    "    FP = torch.sum ((     (input>threshold)  * (1 - target)).float())\n",
    "    FN = torch.sum (((1 - (input>threshold)) * target      ).float())\n",
    "#     print ('TP = ', TP.item(), 'FP = ', FP.item(), 'FN = ', FN.item(), 'N = ', input.shape[0])\n",
    "    return TP / (TP + FP), TP / (TP + FN)\n",
    "\n",
    "def my_precision_TPR_FPR (input, target, threshold):\n",
    "    TP = torch.sum ((     (input>threshold) .float() * target      ).float())\n",
    "    FP = torch.sum ((     (input>threshold) .float() * (1 - target)).float())\n",
    "    FN = torch.sum (((1 - (input>threshold)).float() * target      ).float())\n",
    "    TN = torch.sum (((1 - (input>threshold)).float() * (1 - target)).float())\n",
    "    return TP / (TP + FP), TP / (TP + FN), FP / (FP + TN)\n",
    "\n",
    "def my_TP_FN_FP_TN (input, target, threshold):\n",
    "    matrix = np.zeros((2, 2))\n",
    "    matrix[0, 0] = torch.sum ((     (input>threshold) .float() * target      ).float())\n",
    "    matrix[1, 0] = torch.sum ((     (input>threshold) .float() * (1 - target)).float())\n",
    "    matrix[0, 1] = torch.sum (((1 - (input>threshold)).float() * target      ).float())\n",
    "    matrix[1, 1] = torch.sum (((1 - (input>threshold)).float() * (1 - target)).float())\n",
    "    return matrix / np.sum(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим датасет\n",
    "#### (Может не влезть в оперативку (надо ~ 12Gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVED_DAYS = 64     # ~2 months\n",
    "DAYS_TO_PREDICT_AFTER  = 10\n",
    "DAYS_TO_PREDICT_BEFORE = 50\n",
    "TESTING_DAYS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarthquakeDataset_RNN_Train (Dataset):\n",
    "    def __init__(self, celled_data, heavy_quake_thres):\n",
    "        self.heavy_quake_thres = heavy_quake_thres\n",
    "        self.mean_val = (celled_data>heavy_quake_thres).float().mean(dim=0)\n",
    "        self.data = celled_data[0:\n",
    "                                (celled_data.shape[0] -\n",
    "                                 TESTING_DAYS)]\n",
    "        self.size = (self.data.shape[0] -\n",
    "                     DAYS_TO_PREDICT_BEFORE)\n",
    "        \n",
    "#         print ('self.data :', self.data.shape)\n",
    "#         print ('size      :', self.size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        accurate_pred = ((torch.sum(self.data[(idx +\n",
    "                                               DAYS_TO_PREDICT_AFTER):\n",
    "                                              (idx +\n",
    "                                               DAYS_TO_PREDICT_BEFORE)] > self.heavy_quake_thres,\n",
    "                                    dim=0,\n",
    "                                    keepdim=True).squeeze(0) > 0).float()\n",
    "                          - self.mean_val)\n",
    "        return (self.data[(idx)],\n",
    "                torch.cat([1 - accurate_pred, accurate_pred], dim=0))\n",
    "        \n",
    "\n",
    "class EarthquakeDataset_RNN_Test (Dataset):\n",
    "    def __init__(self, celled_data, heavy_quake_thres):\n",
    "        self.heavy_quake_thres = heavy_quake_thres\n",
    "        self.mean_val = (celled_data>heavy_quake_thres).float().mean(dim=0)\n",
    "        self.data = celled_data[(celled_data.shape[0] -\n",
    "                                 TESTING_DAYS):\n",
    "                                (celled_data.shape[0])]\n",
    "        self.size = (self.data.shape[0] -\n",
    "                     DAYS_TO_PREDICT_BEFORE)\n",
    "        \n",
    "#         print ('self.data :', self.data.shape)\n",
    "#         print ('size      :', self.size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[(idx)],\n",
    "                torch.sum(self.data[(idx +\n",
    "                                     DAYS_TO_PREDICT_AFTER):\n",
    "                                    (idx +\n",
    "                                     DAYS_TO_PREDICT_BEFORE)] > self.heavy_quake_thres,\n",
    "                          dim=0,\n",
    "                          keepdim=True).squeeze(0) > 0)\n",
    "    \n",
    "class EarthquakeDataset_RNN_Test_Cir (Dataset):\n",
    "    def __init__(self, celled_data, celled_data_cir, heavy_quake_thres):\n",
    "        self.heavy_quake_thres = heavy_quake_thres\n",
    "        self.mean_val = (celled_data>heavy_quake_thres).float().mean(dim=0)\n",
    "        \n",
    "        self.data = celled_data[(celled_data.shape[0] -\n",
    "                                 TESTING_DAYS):\n",
    "                                (celled_data.shape[0])]\n",
    "        self.target = celled_data_cir[(celled_data.shape[0] -\n",
    "                                       TESTING_DAYS):\n",
    "                                      (celled_data.shape[0])]\n",
    "        self.size = (self.data.shape[0] -\n",
    "                     DAYS_TO_PREDICT_BEFORE)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[(idx)],\n",
    "                torch.sum(self.target[(idx +\n",
    "                                       DAYS_TO_PREDICT_AFTER):\n",
    "                                      (idx +\n",
    "                                       DAYS_TO_PREDICT_BEFORE)] > self.heavy_quake_thres,\n",
    "                          dim=0,\n",
    "                          keepdim=True).squeeze(0) > 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим саму сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.CONV  = nn.Conv2d    (in_channels,\n",
    "                                   out_channels,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=stride,\n",
    "                                   padding=padding,\n",
    "                                   bias=False)             # think about it later\n",
    "        \n",
    "        self.BNORM =nn.BatchNorm2d(out_channels,\n",
    "                                   eps=1e-05,\n",
    "                                   momentum=0.1,\n",
    "                                   affine=False)\n",
    "#         self.RELU  = nn.ReLU ()\n",
    "        \n",
    "#         self.MAXPOOL = nn.MaxPool2d(3,\n",
    "#                                     stride=1,\n",
    "#                                     padding=1,\n",
    "#                                     dilation=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print ('sizeof(x) = ', x.size())\n",
    "        #print ('sizeof(xprev) = ', xprev.size())    \n",
    "        \n",
    "        x = self.CONV   (x)\n",
    "        x = self.BNORM  (x)\n",
    "#         x = self.RELU   (x)\n",
    "#         x = self.MAXPOOL(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell (nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size=16, hidden_state_size=32):\n",
    "        super(self.__class__,self).__init__()\n",
    "        \n",
    "        self.emb_size = embedding_size\n",
    "        self.  h_size = hidden_state_size\n",
    "        \n",
    "        self.embedding  = ConvBlock (1, embedding_size, 3)\n",
    "        self.RNN_update = nn.Sequential (ConvBlock (hidden_state_size + embedding_size,\n",
    "                                                    hidden_state_size + embedding_size,\n",
    "                                                    kernel_size=3),\n",
    "                                         ConvBlock (hidden_state_size + embedding_size,\n",
    "                                                    hidden_state_size,\n",
    "                                                    kernel_size=3))\n",
    "        self.RNN_to_result = nn.Sequential (ConvBlock (hidden_state_size, \n",
    "                                                       2, \n",
    "                                                       kernel_size=3),\n",
    "                                            nn.Softmax (dim=1))\n",
    "        \n",
    "    def forward (self, x, h_prev):\n",
    "        \n",
    "        x_emb   = self.embedding (x)\n",
    "        x_and_h = torch.cat([x_emb, h_prev], dim=1)\n",
    "#         print (\"x_and_h : \", x_and_h.shape)\n",
    "        h_next  = self.RNN_update(x_and_h)\n",
    "#         print (\"h_prev :\", h_prev.shape)\n",
    "#         print (\"h_next :\", h_next.shape)\n",
    "        \n",
    "        assert h_prev.shape == h_next.shape\n",
    "        \n",
    "        h_next = F.tanh(h_next)\n",
    "        result = self.RNN_to_result(h_next)\n",
    "        return h_next, result\n",
    "    \n",
    "    def init_state (self, batch_size, device=torch.device(\"cpu\")):\n",
    "        return torch.zeros(batch_size,\n",
    "                           self.h_size,\n",
    "                           N_CELLS_HOR, \n",
    "                           N_CELLS_VER,\n",
    "                           requires_grad=False,\n",
    "                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StupidNetCell (nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(self.__class__,self).__init__()\n",
    "        \n",
    "    def forward (self, x, prev_state):\n",
    "        ret = torch.zeros([x.shape[0], 2, N_CELLS_HOR, N_CELLS_VER])\n",
    "        for i in range(x.shape[0]):\n",
    "            ret[i, 0] = freq_map\n",
    "            ret[i, 1] = freq_map\n",
    "        return 0, ret\n",
    "    \n",
    "    def init_state (self, batch_size, device=torch.device(\"cpu\")):\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "![LSTM](./img/LSTM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell (nn.Module):\n",
    "    \n",
    "    def __init__ (self,\n",
    "                  embedding_size=16,\n",
    "                  hidden_state_size=32):\n",
    "        super(self.__class__,self).__init__()\n",
    "        \n",
    "        self.emb_size = embedding_size\n",
    "        self.hid_size = hidden_state_size\n",
    "        \n",
    "#         self.embedding = ConvBlock (1, self.emb_size, kernel_size=3)\n",
    "        self.embedding = nn.Sequential(ConvBlock(1,\n",
    "                                                 self.emb_size,\n",
    "                                                 3),\n",
    "                                       nn.ReLU(),\n",
    "                                       ConvBlock(self.emb_size,\n",
    "                                                 self.emb_size,\n",
    "                                                 3))\n",
    "        self.hidden_to_result = nn.Sequential (ConvBlock (hidden_state_size, \n",
    "                                                          2, \n",
    "                                                          kernel_size=3),\n",
    "                                               nn.Softmax (dim=1))\n",
    "        \n",
    "        self.f_t = nn.Sequential (ConvBlock(self.hid_size + self.emb_size,\n",
    "                                            self.hid_size,\n",
    "                                            3),\n",
    "                                  nn.Sigmoid())\n",
    "        self.i_t = nn.Sequential (ConvBlock(self.hid_size + self.emb_size,\n",
    "                                            self.hid_size,\n",
    "                                            3),\n",
    "                                  nn.Sigmoid())\n",
    "        self.c_t = nn.Sequential (ConvBlock(self.hid_size + self.emb_size,\n",
    "                                            self.hid_size,\n",
    "                                            3),\n",
    "                                  nn.Tanh())\n",
    "        self.o_t = nn.Sequential (ConvBlock(self.hid_size + self.emb_size,\n",
    "                                            self.hid_size,\n",
    "                                            3),\n",
    "                                  nn.Sigmoid())\n",
    "        \n",
    "    def forward (self, x, prev_state):\n",
    "        (prev_c, prev_h) = prev_state\n",
    "        x_emb = self.embedding(x)\n",
    "        \n",
    "        x_and_h = torch.cat([prev_h, x_emb], dim=1)\n",
    "        \n",
    "        f_i = self.f_t(x_and_h)\n",
    "        i_i = self.i_t(x_and_h)\n",
    "        c_i = self.c_t(x_and_h)\n",
    "        o_i = self.o_t(x_and_h)\n",
    "        \n",
    "        next_c = prev_c * f_i + i_i * c_i\n",
    "        next_h = torch.tanh(next_c) * o_i\n",
    "        \n",
    "        assert prev_h.shape == next_h.shape\n",
    "        assert prev_c.shape == next_c.shape\n",
    "        \n",
    "        return (next_c, next_h), self.hidden_to_result(next_h)\n",
    "        \n",
    "    def init_state (self, batch_size, device=torch.device(\"cpu\")):\n",
    "        return (Variable(torch.zeros(batch_size,\n",
    "                                     self.hid_size,\n",
    "                                     N_CELLS_HOR,\n",
    "                                     N_CELLS_VER,\n",
    "                                     device=device)),\n",
    "                Variable(torch.zeros(batch_size,\n",
    "                                     self.hid_size,\n",
    "                                     N_CELLS_HOR,\n",
    "                                     N_CELLS_VER,\n",
    "                                     device=device)))\n",
    "               \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция тренеровки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_crossEntropy(weights, prediction, target):\n",
    "    assert len(weights) == prediction.shape[1]\n",
    "    assert prediction.shape == target.shape\n",
    "    loss = 0\n",
    "    for i in range(prediction.shape[0]):\n",
    "        for j in range(len(weights)):\n",
    "            loss -= weights[j] * torch.sum(target[i, j] * prediction[i, j].log())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network_RNN (RNN_cell,\n",
    "                       device,\n",
    "                       dataloader_train,\n",
    "                       n_cycles=1,\n",
    "                       learning_rate=0.0003,\n",
    "                       earthquake_weight=1.,\n",
    "                       lr_decay=1.):\n",
    "    \n",
    "    loss_massive = []\n",
    "    i = 0\n",
    "    \n",
    "    RNN_cell.to(device)\n",
    "    \n",
    "    weights = torch.tensor([1., earthquake_weight], dtype=torch.float).to(device)\n",
    "#     criterion = nn.CrossEntropyLoss(weights)\n",
    "    \n",
    "    i = 0\n",
    "    for cycle in range(n_cycles):\n",
    "        \n",
    "        optimizer = torch.optim.Adam(RNN_cell.parameters(), lr=learning_rate)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        hid_state = RNN_cell.init_state(batch_size=1, device=device)\n",
    "        for data in tqdm(dataloader_train):\n",
    "            \n",
    "            inputs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "            \n",
    "#             print (\"inputs\", inputs.shape)\n",
    "#             print (\"hid_state\", hid_state.shape)\n",
    "            hid_state, outputs = RNN_cell.forward(inputs, hid_state)\n",
    "            \n",
    "#             loss = criterion(outputs, labels.squeeze(1).long())\n",
    "            loss = my_crossEntropy(weights, outputs, labels)\n",
    "            loss_massive.append(loss.item())\n",
    "#             loss.backward(retain_graph=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "#             print (\"day : \", i, \"loss : \", loss.item())\n",
    "            \n",
    "            if (type(hid_state) == tuple):\n",
    "                for elem in hid_state:\n",
    "                    elem.detach_()\n",
    "            else:\n",
    "                hid_state.detach_()\n",
    "            \n",
    "#             if (i)%100==0:\n",
    "#                 clear_output(True)\n",
    "#                 print (\"Done :\", i, \"/\", dataloader_train.__len__())\n",
    "#                 plt.plot(loss_massive,label='loss')\n",
    "#                 plt.legend()\n",
    "#                 plt.show()\n",
    "            i += 1\n",
    "        learning_rate /= lr_decay\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CYCLES = 1\n",
    "LEARNING_RATE = 0.0003\n",
    "LR_DECAY = 10.\n",
    "EARTHQUAKE_WEIGHT = 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 16\n",
    "HID_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RNN_cell = RNNCell()\n",
    "# RNN_cell = LSTMCell(embedding_size    = EMB_SIZE,\n",
    "#                     hidden_state_size = HID_SIZE)\n",
    "# train_network_RNN (RNN_cell,\n",
    "#                    DEVICE,\n",
    "#                    earthquakes_dataloader_train,\n",
    "#                    n_cycles=N_CYCLES,\n",
    "#                    learning_rate=LEARNING_RATE,\n",
    "#                    earthquake_weight=EARTHQUAKE_WEIGHT,\n",
    "#                    lr_decay=LR_DECAY\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_quality (RNN_cell,\n",
    "                   device,\n",
    "                   dataloader_test,\n",
    "                   n_dots=501,\n",
    "                   info_file=None):\n",
    "    \n",
    "    prediction = torch.zeros(dataloader_test.__len__(),  N_CELLS_HOR, N_CELLS_VER,\n",
    "                             device=device,\n",
    "                             dtype=torch.float)\n",
    "    prediction.detach_()\n",
    "    target     = torch.zeros(dataloader_test.__len__(), N_CELLS_HOR, N_CELLS_VER,\n",
    "                             device=device,\n",
    "                             dtype=torch.float)\n",
    "    target.detach_()\n",
    "       \n",
    "    RNN_cell.to(device)\n",
    "\n",
    "    hid_state = RNN_cell.init_state(batch_size=1, device=device)\n",
    "    if (type(hid_state) == tuple):\n",
    "        for elem in hid_state:\n",
    "            elem.detach_()\n",
    "    else:\n",
    "        hid_state.detach_()\n",
    "        \n",
    "    for data in tqdm(dataloader_test):\n",
    "\n",
    "        inputs = data[0].to(device)\n",
    "        labels = data[1].to(device).float()\n",
    "\n",
    "        hid_state, outputs = RNN_cell.forward(inputs, hid_state)\n",
    "        \n",
    "        prediction[i] = outputs[:, 1, :, :]\n",
    "        target    [i] = labels.squeeze(0)\n",
    "    \n",
    "        if (type(hid_state) == tuple):\n",
    "            for elem in hid_state:\n",
    "                elem.detach_()\n",
    "        else:\n",
    "            hid_state.detach_()\n",
    "        prediction.detach_()\n",
    "        target    .detach_()\n",
    "        \n",
    "    assert prediction.shape == target.shape\n",
    "    prediction = prediction [10:prediction.shape[0]]  # cutting peace of data because\n",
    "    target     = target     [10:target    .shape[0]]  # hidden state might be not good\n",
    "    \n",
    "    print (\"ROC_AUC_score = \", end='')\n",
    "    ROC_AUC_score = roc_auc_score(np.array(target    .view(-1).cpu()),\n",
    "                                  np.array(prediction.view(-1).cpu()))\n",
    "    print (ROC_AUC_score)\n",
    "    if (SAVE_INFO):\n",
    "        print ('ROC_AUC               =', ROC_AUC_score, file=info_file)\n",
    "    \n",
    "    print (\"AVG_precision_score = \", end='')\n",
    "    AVG_precision_score = average_precision_score(np.array(target    .view(-1).cpu()),\n",
    "                                                  np.array(prediction.view(-1).cpu()))\n",
    "    print (AVG_precision_score)\n",
    "    if (SAVE_INFO):\n",
    "        print ('Average_precision     =', AVG_precision_score, file=info_file)\n",
    "        \n",
    "    print ('\\n=======================')\n",
    "    \n",
    "    for threshold in (0.2, 0.4, 0.6, 0.8):\n",
    "        print ('Threshold = ', threshold)\n",
    "        print ('-----------------------')\n",
    "        print (my_TP_FN_FP_TN(prediction, target, threshold))\n",
    "        print ('=======================')\n",
    "    \n",
    "    if SAVE_INFO:\n",
    "        print ('\\n=======================', file=info_file)\n",
    "    \n",
    "        for threshold in (0.2, 0.4, 0.6, 0.8):\n",
    "            print ('Threshold = ', threshold                    , file=info_file)\n",
    "            print ('-----------------------'                    , file=info_file)\n",
    "            print (my_TP_FN_FP_TN(prediction, target, threshold), file=info_file)\n",
    "            print ('======================='                    , file=info_file)\n",
    "    \n",
    "    threshold_massive = torch.linspace (0, 1, n_dots, dtype=torch.float, device=device)\n",
    "    \n",
    "#     precision = np.zeros(n_dots)\n",
    "#     recall    = np.zeros(n_dots)\n",
    "#     FPR       = np.zeros(n_dots)\n",
    "\n",
    "    precision_massive = []\n",
    "    recall_massive    = []\n",
    "    FPR_massive       = []\n",
    "    \n",
    "    for threshold in tqdm(threshold_massive):\n",
    "        precision, recall, FPR = my_precision_TPR_FPR(prediction, target, threshold)\n",
    "        precision_massive.append(precision.item())\n",
    "        recall_massive   .append(recall.item())\n",
    "        FPR_massive      .append(FPR.item())\n",
    "    \n",
    "    # plot 1 precision\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "    axes.plot(np.array(threshold_massive.cpu()), precision_massive, color='green', marker='^')\n",
    "\n",
    "    axes.set_xlabel('threshold')\n",
    "    axes.set_ylabel('precision')\n",
    "\n",
    "    if (SAVE_INFO == True):\n",
    "        plt.savefig(EXPERIMENT_DIR + 'Precision_from_threshold.png', format='png', dpi=100)\n",
    "    plt.show()\n",
    "    \n",
    "    # plot 2 recall\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "    axes.plot(np.array(threshold_massive.cpu()), recall_massive, color='green', marker='^')\n",
    "\n",
    "    axes.set_xlabel('threshold')\n",
    "    axes.set_ylabel('recall')\n",
    "    \n",
    "    if (SAVE_INFO == True):\n",
    "        plt.savefig(EXPERIMENT_DIR + 'Recall_from_threshold.png', format='png', dpi=100)\n",
    "    plt.show()\n",
    "    \n",
    "    # plot 3 ROC-curve\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "    axes.plot(FPR_massive, recall_massive, 'orange', marker = '^')\n",
    "    axes.plot (range(2), range(2), 'grey', ls='--')\n",
    "\n",
    "    axes.set_xlabel('FPR')\n",
    "    axes.set_ylabel('TPR (recall)')\n",
    "    axes.set_title('ROC-curve')\n",
    "\n",
    "    if (SAVE_INFO == True):\n",
    "        plt.savefig(EXPERIMENT_DIR + 'ROC_curve.png', format='png', dpi=100)\n",
    "    plt.show()\n",
    "    \n",
    "    # plot 4 precision-recall-curve\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "    axes.plot(recall_massive, precision_massive, 'orange', marker = '^')\n",
    "\n",
    "    axes.set_xlabel('Recall')\n",
    "    axes.set_ylabel('Precision')\n",
    "    axes.set_title('Precision_Recall_curve')\n",
    "\n",
    "    if (SAVE_INFO == True):\n",
    "        plt.savefig(EXPERIMENT_DIR + 'Precision_Recall_curve.png', format='png', dpi=100)\n",
    "    plt.show()\n",
    "    \n",
    "    return ROC_AUC_score, AVG_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_F1_score(target, prediction):\n",
    "    max_F1 = 0\n",
    "    for i in np.linspace(0, 1, 21):\n",
    "        F1 = f1_score(target, prediction>i)\n",
    "        max_F1 = max(max_F1, F1)\n",
    "    \n",
    "    return max_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(RNN_cell,\n",
    "                      device,\n",
    "                      dataloader_test):\n",
    "    \n",
    "    prediction = torch.zeros(dataloader_test.__len__(),  N_CELLS_HOR, N_CELLS_VER,\n",
    "                             device=device,\n",
    "                             dtype=torch.float)\n",
    "    prediction.detach_()\n",
    "    target     = torch.zeros(dataloader_test.__len__(), N_CELLS_HOR, N_CELLS_VER,\n",
    "                             device=device,\n",
    "                             dtype=torch.float)\n",
    "    target.detach_()\n",
    "       \n",
    "    RNN_cell.to(device)\n",
    "\n",
    "    hid_state = RNN_cell.init_state(batch_size=1, device=device)\n",
    "    if (type(hid_state) == tuple):\n",
    "        for elem in hid_state:\n",
    "            elem.detach_()\n",
    "    else:\n",
    "        hid_state.detach_()\n",
    "    \n",
    "    i = 0\n",
    "    for data in tqdm(dataloader_test):\n",
    "\n",
    "        inputs = data[0].to(device)\n",
    "        labels = data[1].to(device).float()\n",
    "\n",
    "        hid_state, outputs = RNN_cell.forward(inputs, hid_state)\n",
    "        \n",
    "        prediction[i] = outputs[:, 1, :, :]\n",
    "        target    [i] = labels.squeeze(0)\n",
    "    \n",
    "        if (type(hid_state) == tuple):\n",
    "            for elem in hid_state:\n",
    "                elem.detach_()\n",
    "        else:\n",
    "            hid_state.detach_()\n",
    "        prediction.detach_()\n",
    "        target    .detach_()\n",
    "        i += 1\n",
    "    \n",
    "    assert prediction.shape == target.shape\n",
    "    prediction = prediction [10:prediction.shape[0]]  # cutting peace of data because\n",
    "    target     = target     [10:target    .shape[0]]  # hidden state might be not good\n",
    "    \n",
    "    prediction += dataloader_test.dataset.mean_val.to(device)\n",
    "    \n",
    "    ROC_AUC_score = roc_auc_score(np.array(target    .view(-1).cpu()),\n",
    "                                  np.array(prediction.view(-1).cpu()))\n",
    "    \n",
    "    \n",
    "    AVG_precision_score = average_precision_score(np.array(target    .view(-1).cpu()),\n",
    "                                                  np.array(prediction.view(-1).cpu()))\n",
    "    \n",
    "    F1_score = max_F1_score(np.array(target    .view(-1).cpu()),\n",
    "                            np.array(prediction.view(-1).cpu()))\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(np.array(target    .view(-1).cpu()),\n",
    "                                                  np.array(prediction.view(-1).cpu()))\n",
    "    \n",
    "    prec_rec_AUC = auc (recall, precision)\n",
    "\n",
    "    return ROC_AUC_score, AVG_precision_score, F1_score, prec_rec_AUC\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pos_class_portion (device,\n",
    "                                 dataloader):\n",
    "\n",
    "    n_pos = 0\n",
    "    n_all = 0\n",
    "    \n",
    "    for data in tqdm(dataloader):\n",
    "\n",
    "        labels = data[1][:, 1, :, :].to(device).float() + dataloader.dataset.mean_val.to(device)\n",
    "        \n",
    "        n_pos += torch.sum(labels).item()\n",
    "        n_all += (labels.shape[0] *\n",
    "                  labels.shape[1] *\n",
    "                  labels.shape[2])\n",
    "\n",
    "    return (n_pos / (n_all - n_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_results (thresholds_range, n_launches):\n",
    "    results = pd.DataFrame({\"threshold\"       :[0 for i in range(len(thresholds_range))],\n",
    "                            \"portion\"         :[[0]*n_launches for i in range(len(thresholds_range))],\n",
    "                            \"weight\"          :[[0]*n_launches for i in range(len(thresholds_range))],\n",
    "                            \"avg_prec\"        :[[0]*n_launches for i in range(len(thresholds_range))],\n",
    "                            \"roc_auc\"         :[[0]*n_launches for i in range(len(thresholds_range))],\n",
    "                            \"max_F1\"          :[[0]*n_launches for i in range(len(thresholds_range))],\n",
    "                            \"prec_rec_auc\"    :[[0]*n_launches for i in range(len(thresholds_range))],\n",
    "                            \"ext_avg_prc\"     :[[0]*n_launches for i in range(len(thresholds_range))],\n",
    "                            \"ext_roc_auc\"     :[[0]*n_launches for i in range(len(thresholds_range))],\n",
    "                            \"ext_max_F1\"      :[[0]*n_launches for i in range(len(thresholds_range))],\n",
    "                            \"ext_prec_rec_auc\":[[0]*n_launches for i in range(len(thresholds_range))]})\n",
    "    for i in range(len(thresholds_range)):\n",
    "        results.loc[i, \"threshold\"]        = thresholds_range[i]\n",
    "        \n",
    "#         results.loc[i, \"avg_prec\"]         = []\n",
    "#         results.loc[i, \"roc_auc\"]          = []\n",
    "#         results.loc[i, \"max_F1\"]           = []\n",
    "#         results.loc[i, \"prec_rec_auc\"]     = []\n",
    "        \n",
    "#         results.loc[i, \"ext_avg_prc\"]      = []\n",
    "#         results.loc[i, \"ext_roc_auc\"]      = []\n",
    "#         results.loc[i, \"ext_max_F1\"]       = []\n",
    "#         results.loc[i, \"ext_prec_rec_auc\"] = []\n",
    "    return results\n",
    "\n",
    "def calc_metrics_from_threshold (RNN_class,\n",
    "                                device,\n",
    "                                thresholds_range,\n",
    "                                learning_rate=0.0003,\n",
    "                                n_cycles=1,\n",
    "                                lr_decay=1.,\n",
    "                                n_launches=1):\n",
    "    \n",
    "    results = init_results(thresholds_range, n_launches)\n",
    "    print (results)\n",
    "    \n",
    "    i = 0\n",
    "    for heavy_threshold in tqdm(thresholds_range):\n",
    "        os.mkdir(\"ready/\"+str(i))\n",
    "        \n",
    "        dataset_train    = EarthquakeDataset_RNN_Train    (celled_data,\n",
    "                                                           heavy_threshold)\n",
    "        dataset_test     = EarthquakeDataset_RNN_Test     (celled_data,\n",
    "                                                           heavy_threshold)\n",
    "        dataset_test_cir = EarthquakeDataset_RNN_Test_Cir (celled_data,\n",
    "                                                           celled_data_cir,\n",
    "                                                           heavy_threshold)\n",
    "        \n",
    "        dataloader_train = DataLoader(dataset_train,\n",
    "                                      batch_size=1,\n",
    "                                      shuffle=False,\n",
    "                                      num_workers=1)\n",
    "        dataloader_test  = DataLoader(dataset_test,\n",
    "                                      batch_size=1,\n",
    "                                      shuffle=False,\n",
    "                                      num_workers=1)\n",
    "        dataloader_test_cir = DataLoader(dataset_test_cir,\n",
    "                                         batch_size=1,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=1)\n",
    "        \n",
    "        portion = calculate_pos_class_portion(device,\n",
    "                                              dataloader_train)\n",
    "        results.loc[i, \"portion\"] = portion\n",
    "        weight = 1 / portion\n",
    "        results.loc[i, \"weight\"]  = weight\n",
    "        \n",
    "        print (\"threshold = \", heavy_threshold,\n",
    "               \"portion = \", portion,\n",
    "               \"weight = \", weight)\n",
    "        \n",
    "        for k in range(n_launches):\n",
    "            RNN_cell = RNN_class(embedding_size    = EMB_SIZE,\n",
    "                                 hidden_state_size = HID_SIZE)\n",
    "#             RNN_cell = RNN_class()\n",
    "            train_network_RNN (RNN_cell,\n",
    "                               device,\n",
    "                               dataloader_train,\n",
    "                               n_cycles=n_cycles,\n",
    "                               learning_rate=learning_rate,\n",
    "                               earthquake_weight=weight,\n",
    "                               lr_decay=lr_decay)\n",
    "            RNN_cell.eval()\n",
    "            (roc_auc,\n",
    "             avg_prec,\n",
    "             F1,\n",
    "             prec_rec_auc)     = calculate_metrics(RNN_cell,\n",
    "                                                   device,\n",
    "                                                   dataloader_test)\n",
    "            (ext_roc_auc,\n",
    "             ext_avg_prec,\n",
    "             ext_F1,\n",
    "             ext_prec_rec_auc) = calculate_metrics(RNN_cell,\n",
    "                                                   device,\n",
    "                                                   dataloader_test_cir)\n",
    "    \n",
    "            results.loc[i, \"avg_prec\"]         [k] = avg_prec\n",
    "            results.loc[i, \"roc_auc\"]          [k] = roc_auc\n",
    "            results.loc[i, \"max_F1\"]           [k] = F1\n",
    "            results.loc[i, \"prec_rec_auc\"]     [k] = prec_rec_auc\n",
    "\n",
    "            results.loc[i, \"ext_avg_prc\"]      [k] = ext_avg_prec\n",
    "            results.loc[i, \"ext_roc_auc\"]      [k] = ext_roc_auc\n",
    "            results.loc[i, \"ext_max_F1\"]       [k] = ext_F1\n",
    "            results.loc[i, \"ext_prec_rec_auc\"] [k] = ext_prec_rec_auc\n",
    "            \n",
    "            os.mkdir(\"ready/\"+str(i)+\"/\"+str(k))\n",
    "            results.to_csv(\"ready/result\")\n",
    "        \n",
    "        print (results)\n",
    "        i += 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    threshold                   portion                    weight  \\\n",
      "0         4.5  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "1         4.9  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "2         5.0  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "3         5.1  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "4         5.2  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "5         5.3  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "6         5.4  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "7         5.5  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "8         5.6  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "9         5.7  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "10        5.8  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "11        5.9  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "12        6.0  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "13        6.1  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "\n",
      "                    avg_prec                   roc_auc  \\\n",
      "0   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "1   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "2   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "3   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "4   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "5   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "6   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "7   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "8   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "9   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "10  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "11  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "12  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "13  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "\n",
      "                      max_F1              prec_rec_auc  \\\n",
      "0   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "1   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "2   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "3   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "4   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "5   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "6   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "7   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "8   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "9   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "10  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "11  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "12  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "13  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "\n",
      "                 ext_avg_prc               ext_roc_auc  \\\n",
      "0   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "1   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "2   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "3   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "4   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "5   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "6   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "7   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "8   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "9   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "10  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "11  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "12  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "13  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "\n",
      "                  ext_max_F1          ext_prec_rec_auc  \n",
      "0   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "1   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "2   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "3   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "4   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "5   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "6   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "7   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "8   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "9   [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "10  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "11  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "12  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "13  [0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1906215240554a208f8d7a347c60c8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72d417508f84fe49c80f032783bc112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8541), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold =  4.5 portion =  0.00030291575545817026 weight =  3301.2478947734708\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68add9215476429fbdb870cbc7ae241a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8541), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb54dc0904b459c8b953c579ef2009f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=950), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romakail/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377cfbdd859e4cb39098212612133486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=950), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romakail/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c2544927ed4fccad9a89b044001f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8541), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945a546a10904e729fcb44d204b68fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=950), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romakail/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd48a25c73946fdb7adef6c7a17140c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=950), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romakail/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99a36f08dcc488dae8e810ab0c72f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8541), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08793c68ae4848688f2901bfdaa9727b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=950), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romakail/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617ffe31987c4a33acc4219163033454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=950), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romakail/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488a2191886d4784908a1cf94cbad5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8541), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de65a4a1126e46b7a58bf4bb03e9bef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=950), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romakail/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94023a039179482e9b291c1e53fb176b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=950), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romakail/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4df761a9b147c4b5943a28e9e8dfec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8541), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7e6f4f74a3460aa39383b7978f3dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=950), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romakail/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4515227ec04dff96895704d4feef77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=950), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romakail/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bab3f69aee476782ecd9cb4ea66b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8541), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc75f868a044d1bba1ccf2632a4e32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=950), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romakail/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e810775505c644948ec9257df62d5671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=950), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romakail/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c2fe4554ea4da1b1d1514ad51aaf25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8541), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf8b5cabbe14e93b666a981885e8b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=950), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = calc_metrics_from_threshold (LSTMCell,\n",
    "                                       DEVICE,\n",
    "                                       (4.5, 5.7, 5.8, 5.9, 6.0, 6.1),\n",
    "                                       learning_rate=LEARNING_RATE,\n",
    "                                       n_cycles=N_CYCLES,\n",
    "                                       lr_decay=LR_DECAY,\n",
    "                                       n_launches=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"New_metric_f_threshold_imp_const_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"Metric_threshold_dependency_8_launches.csv\")\n",
    "baseline = pd.read_csv(\"Metrics_from_threshold_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = baseline.sort_values(by=\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = results.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(results.shape[0]):\n",
    "    \n",
    "    parsed_1 = results.loc[i, \"avg_prec\"].replace('[', '').replace(']', '')\n",
    "    parsed_2 = [float(x) for x in parsed_1.split(',')]\n",
    "    results.loc[i, \"avg_prec_mean\"] = np.array(parsed_2).mean()\n",
    "    results.loc[i, \"avg_prec_std\" ] = np.array(parsed_2).std()\n",
    "    \n",
    "    parsed_1 = results.loc[i, \"roc_auc\"].replace('[', '').replace(']', '')\n",
    "    parsed_2 = [float(x) for x in parsed_1.split(',')]\n",
    "    results.loc[i, \"roc_auc_mean\"] = np.array(parsed_2).mean()\n",
    "    results.loc[i, \"roc_auc_std\" ] = np.array(parsed_2).std()\n",
    "    \n",
    "    parsed_1 = results.loc[i, \"max_F1\"].replace('[', '').replace(']', '')\n",
    "    parsed_2 = [float(x) for x in parsed_1.split(',')]\n",
    "    results.loc[i, \"max_F1_mean\"] = np.array(parsed_2).mean()\n",
    "    results.loc[i, \"max_F1_std\" ] = np.array(parsed_2).std()\n",
    "    \n",
    "    parsed_1 = results.loc[i, \"prec_rec_auc\"].replace('[', '').replace(']', '')\n",
    "    parsed_2 = [float(x) for x in parsed_1.split(',')]\n",
    "    results.loc[i, \"prec_rec_auc_mean\"] = np.array(parsed_2).mean()\n",
    "    results.loc[i, \"prec_rec_auc_std\" ] = np.array(parsed_2).std()\n",
    "    \n",
    "    parsed_1 = results.loc[i, \"ext_avg_prc\"].replace('[', '').replace(']', '')\n",
    "    parsed_2 = [float(x) for x in parsed_1.split(',')]\n",
    "    results.loc[i, \"ext_avg_prc_mean\"] = np.array(parsed_2).mean()\n",
    "    results.loc[i, \"ext_avg_prc_std\" ] = np.array(parsed_2).std()\n",
    "    \n",
    "    parsed_1 = results.loc[i, \"ext_roc_auc\"].replace('[', '').replace(']', '')\n",
    "    parsed_2 = [float(x) for x in parsed_1.split(',')]\n",
    "    results.loc[i, \"ext_roc_auc_mean\"] = np.array(parsed_2).mean()\n",
    "    results.loc[i, \"ext_roc_auc_std\" ] = np.array(parsed_2).std()\n",
    "    \n",
    "    parsed_1 = results.loc[i, \"ext_max_F1\"].replace('[', '').replace(']', '')\n",
    "    parsed_2 = [float(x) for x in parsed_1.split(',')]\n",
    "    results.loc[i, \"ext_max_F1_mean\"] = np.array(parsed_2).mean()\n",
    "    results.loc[i, \"ext_max_F1_std\" ] = np.array(parsed_2).std()\n",
    "    \n",
    "    parsed_1 = results.loc[i, \"ext_prec_rec_auc\"].replace('[', '').replace(']', '')\n",
    "    parsed_2 = [float(x) for x in parsed_1.split(',')]\n",
    "    results.loc[i, \"ext_prec_rec_auc_mean\"] = np.array(parsed_2).mean()\n",
    "    results.loc[i, \"ext_prec_rec_auc_std\" ] = np.array(parsed_2).std()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 1 portion\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "axes.plot(results[\"threshold\"], results[\"portion\"], color='green', marker='.')\n",
    "\n",
    "axes.set_xlabel('threshold')\n",
    "axes.set_ylabel('portion')\n",
    "\n",
    "# plot 2 precision\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "plt.axis([4.6, 6.0, 0, 0.006])\n",
    "# axes.plot(results[\"threshold\"], results[\"avg_prec\"     ], color='green', marker='.')\n",
    "\n",
    "axes.errorbar(results[\"threshold\"],\n",
    "              results[\"avg_prec_mean\"],\n",
    "              yerr=results['avg_prec_std'],\n",
    "              color='green')\n",
    "axes.plot(baseline[\"threshold\"], baseline[\"avg_prec_base\"], color='blue' , marker='.')\n",
    "\n",
    "\n",
    "axes.set_xlabel('threshold')\n",
    "axes.set_ylabel('average_precision')\n",
    "\n",
    "# plot 3 roc_auc\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "# axes.plot(results[\"threshold\"], results[\"roc_auc\"     ], color='green', marker='.')\n",
    "\n",
    "axes.errorbar(results[\"threshold\"],\n",
    "              results[\"roc_auc_mean\"],\n",
    "              yerr=results['roc_auc_std'],\n",
    "              color='green')\n",
    "\n",
    "axes.plot(baseline[\"threshold\"], baseline[\"roc_auc_base\"], color='blue' , marker='.')\n",
    "\n",
    "axes.set_xlabel('threshold')\n",
    "axes.set_ylabel('roc_auc')\n",
    "\n",
    "# plot 4 F1_score\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "# axes.plot(results[\"threshold\"], results[\"F1 (0.5)\"     ], color='green', marker='.')\n",
    "\n",
    "axes.errorbar(results[\"threshold\"],\n",
    "              results[\"max_F1_mean\"],\n",
    "              yerr=results['max_F1_std'],\n",
    "              color='green',\n",
    "              label='both limits')\n",
    "\n",
    "axes.plot(baseline[\"threshold\"], baseline[\"F1 (0.5)_base\"], color='blue' , marker='.')\n",
    "\n",
    "\n",
    "axes.set_xlabel('threshold')\n",
    "axes.set_ylabel('F1_score')\n",
    "\n",
    "# plot 5 precision_recall auc\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "# axes.plot(results[\"threshold\"], results[\"prec_rec_auc\"     ], color='green', marker='.')\n",
    "\n",
    "axes.errorbar(results[\"threshold\"],\n",
    "              results[\"prec_rec_auc_mean\"],\n",
    "              yerr=results['prec_rec_auc_std'],\n",
    "              color='green',\n",
    "              label='both limits')\n",
    "\n",
    "axes.plot(baseline[\"threshold\"], baseline[\"prec_rec_auc_base\"], color='blue' , marker='.')\n",
    "\n",
    "\n",
    "axes.set_xlabel('threshold')\n",
    "axes.set_ylabel('precision_recall_auc')\n",
    "\n",
    "# # plot 1 precision\n",
    "# fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "# axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "# axes.plot(results[\"threshold\"], results[\"avg_prec\"], color='green', marker='.')\n",
    "\n",
    "# axes.set_xlabel('threshold')\n",
    "# axes.set_ylabel('average_precision')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
